{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2.2: Detecting framenet events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrameNet is a database about situation semantics developed at Berkeley University under the leadership of Fillmore:\n",
    "\n",
    "https://framenet.icsi.berkeley.edu\n",
    "\n",
    "FrameNet provides over a thousand frames that represent conceptual schemata for events involving participants in certain roles.\n",
    "\n",
    "In this notebook, we are going to use the FrameNet module inside the NLTK package to assign frames to words. Words can evoke multiple frames and FrameNet does not provide frames for every word in the English language. Another problem is that FrameNet does not provide frame distributions. We therefore do no know which frames are more likely than others, nor do we know in which texts frames are more dominant. In this notebook, we are going to use trick to learn the dominant frames for your text collection.\n",
    "\n",
    "These are the steps described in this notebook:\n",
    "\n",
    "* obtain predicates and nominal heads from SpaCy\n",
    "* look them up in FrameNet through the NLTK toolkit\n",
    "* determine the dominant frames for a data set\n",
    "* use the dominant frames to assign frames to a document collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FrameNet in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume you have NLTK already installed. To use the FrameNet module, you need to download FrameNet within it. Run the following cell to download it within NLTK. If you have already done this before, FrameNet is included in NLTK and you do not need to download it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0bfa60f5a169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'framenet_v17'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 )\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Look up the requested collection or package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Error loading %s: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \"\"\"Return the ``Package`` or ``Collection`` record for the\n\u001b[1;32m   1018\u001b[0m            given item.\"\"\"\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_update_index\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;31m# Download the index file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         self._index = nltk.internals.ElementWrapper(\n\u001b[0;32m--> 962\u001b[0;31m             \u001b[0mElementTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m         )\n\u001b[1;32m    964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \"\"\"\n\u001b[1;32m   1196\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# It can be used to parse the whole source without feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;31m# it with chunks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the install was succesful, the following code cell should work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After succesful download you can comment out the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1221"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import framenet as fn\n",
    "len(fn.frames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how many different frames there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some instructions how to use FrameNet in NLTK although they are quote sparse:\n",
    "\n",
    "http://www.nltk.org/howto/framenet.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<frame ID=590 name=Killing>]\n"
     ]
    }
   ],
   "source": [
    "### get the frame identifier for a specific Frame\n",
    "print(fn.frames('Killing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<frame ID=262 name=Abounding_with>, <frame ID=59 name=Filling>, ...]\n"
     ]
    }
   ],
   "source": [
    "### get the frames for a specific lemma\n",
    "print(fn.frames_by_lemma('inject'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<frame ID=239 name=Medical_conditions>, <frame ID=257 name=Medical_instruments>, ...]\n"
     ]
    }
   ],
   "source": [
    "### get frames with the substring 'medical' regardless of case\n",
    "print(fn.frames(r'(?i)medical'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cBy': 'ChW',\n",
       " 'cDate': '02/07/2001 04:12:13 PST Wed',\n",
       " 'name': 'Filling',\n",
       " 'ID': 59,\n",
       " '_type': 'frame',\n",
       " 'definition': \"These are words relating to filling containers and covering areas with some thing, things or substance, the Theme. The area or container can appear as the direct object with all these verbs, and is designated Goal because it is the goal of motion of the Theme. Corresponding to its nuclear argument status, it is also affected in some crucial way, unlike goals in other frames.  'Lionel Hutz coated the wall with paint. '\",\n",
       " 'definitionMarkup': '<def-root>These are words relating to filling containers and covering areas with some thing, things or substance, the <fen>Theme</fen>. The area or container can appear as the direct object with all these verbs, and is designated <fen>Goal</fen> because it is the goal of motion of the <fen>Theme</fen>. Corresponding to its nuclear argument status, it is also affected in some crucial way, unlike goals in other frames.\\n <ex><fex name=\"Agent\">Lionel Hutz</fex> <t>coated</t> <fex name=\"Goal\">the wall</fex> <fex name=\"Theme\">with paint</fex>. </ex></def-root>',\n",
       " 'FE': {'Agent': <fe ID=253 name=Agent>, 'Cause': <fe ID=9897 name=Cause>, 'Degree': <fe ID=840 name=Degree>, 'Depictive': <fe ID=841 name=Depictive>, 'Explanation': <fe ID=1948 name=Explanation>, 'Goal': <fe ID=257 name=Goal>, 'Instrument': <fe ID=1952 name=Instrument>, 'Manner': <fe ID=844 name=Manner>, 'Means': <fe ID=845 name=Means>, 'Path': <fe ID=256 name=Path>, 'Place': <fe ID=1950 name=Place>, 'Purpose': <fe ID=1951 name=Purpose>, 'Result': <fe ID=846 name=Result>, 'Source': <fe ID=255 name=Source>, 'Subregion': <fe ID=3579 name=Subregion>, 'Theme': <fe ID=254 name=Theme>, 'Time': <fe ID=1949 name=Time>},\n",
       " 'FEcoreSets': [],\n",
       " 'lexUnit': {'accessorize.v': <lu ID=17364 name=accessorize.v>, 'adorn.v': <lu ID=1007 name=adorn.v>, 'anoint.v': <lu ID=1008 name=anoint.v>, 'asphalt.v': <lu ID=3900 name=asphalt.v>, 'brush.v': <lu ID=2584 name=brush.v>, 'butter.v': <lu ID=3894 name=butter.v>, 'coat.v': <lu ID=3904 name=coat.v>, 'cover.v': <lu ID=1009 name=cover.v>, 'cram.v': <lu ID=2580 name=cram.v>, 'crowd.v': <lu ID=6919 name=crowd.v>, 'dab.v': <lu ID=2577 name=dab.v>, 'daub.v': <lu ID=2578 name=daub.v>, 'douse.v': <lu ID=3928 name=douse.v>, 'drape.v': <lu ID=2576 name=drape.v>, 'dress.v': <lu ID=15822 name=dress.v>, 'drizzle.v': <lu ID=2598 name=drizzle.v>, 'dust.v': <lu ID=1010 name=dust.v>, 'embellish.v': <lu ID=7131 name=embellish.v>, 'fill.v': <lu ID=3908 name=fill.v>, 'flood.v': <lu ID=5205 name=flood.v>, 'gild.v': <lu ID=7127 name=gild.v>, 'glaze.v': <lu ID=7128 name=glaze.v>, 'hang.v': <lu ID=2585 name=hang.v>, 'heap.v': <lu ID=2599 name=heap.v>, 'inject.v': <lu ID=2579 name=inject.v>, 'jam.v': <lu ID=2625 name=jam.v>, 'load.v': <lu ID=1011 name=load.v>, 'pack.v': <lu ID=1012 name=pack.v>, 'paint.v': <lu ID=7108 name=paint.v>, 'panel.v': <lu ID=7138 name=panel.v>, 'pave.v': <lu ID=7017 name=pave.v>, 'pile.v': <lu ID=2612 name=pile.v>, 'plank.v': <lu ID=15775 name=plank.v>, 'plant.v': <lu ID=2626 name=plant.v>, 'plaster.v': <lu ID=2575 name=plaster.v>, 'pump.v': <lu ID=2614 name=pump.v>, 'scatter.v': <lu ID=2627 name=scatter.v>, 'seed.v': <lu ID=2582 name=seed.v>, 'shower.v': <lu ID=2592 name=shower.v>, 'smear.v': <lu ID=1013 name=smear.v>, 'sow.v': <lu ID=2581 name=sow.v>, 'spatter.v': <lu ID=2586 name=spatter.v>, 'splash.v': <lu ID=2587 name=splash.v>, 'splatter.v': <lu ID=2588 name=splatter.v>, 'spray.v': <lu ID=2589 name=spray.v>, 'spread.v': <lu ID=1014 name=spread.v>, 'sprinkle.v': <lu ID=2590 name=sprinkle.v>, 'squirt.v': <lu ID=2591 name=squirt.v>, 'strew.v': <lu ID=3925 name=strew.v>, 'stuff.v': <lu ID=1015 name=stuff.v>, 'suffuse.v': <lu ID=3907 name=suffuse.v>, 'surface.v': <lu ID=3901 name=surface.v>, 'tile.v': <lu ID=3902 name=tile.v>, 'varnish.v': <lu ID=7107 name=varnish.v>, 'wallpaper.v': <lu ID=3903 name=wallpaper.v>, 'wash.v': <lu ID=15630 name=wash.v>, 'wax.v': <lu ID=15613 name=wax.v>, 'wrap.v': <lu ID=1016 name=wrap.v>, 'yoke.v': <lu ID=15779 name=yoke.v>},\n",
       " 'semTypes': [],\n",
       " 'frameRelations': [<Parent=Container_focused_placing -- Inheritance -> Child=Filling>, <MainEntry=Cause_motion -- See_also -> ReferringEntry=Filling>, ...],\n",
       " 'URL': 'https://framenet2.icsi.berkeley.edu/fnReports/data/frame/Filling.xml'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get a specific frame through its identifier\n",
    "f = fn.frame(59)\n",
    "### check what properties and functions are provided for a frame\n",
    "dict(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 59\n",
      "FRAME: Filling\n",
      "DEFINITION These are words relating to filling containers and covering areas with some thing, things or substance, the Theme. The area or container can appear as the direct object with all these verbs, and is designated Goal because it is the goal of motion of the Theme. Corresponding to its nuclear argument status, it is also affected in some crucial way, unlike goals in other frames.  'Lionel Hutz coated the wall with paint. '\n",
      "\n",
      "LEXICAL UNITS:\n",
      "adorn.v\n",
      "anoint.v\n",
      "cover.v\n",
      "dust.v\n",
      "load.v\n",
      "pack.v\n",
      "smear.v\n",
      "spread.v\n",
      "stuff.v\n",
      "wrap.v\n",
      "plaster.v\n",
      "drape.v\n",
      "dab.v\n",
      "daub.v\n",
      "inject.v\n",
      "cram.v\n",
      "sow.v\n",
      "seed.v\n",
      "brush.v\n",
      "hang.v\n",
      "spatter.v\n",
      "splash.v\n",
      "splatter.v\n",
      "spray.v\n",
      "sprinkle.v\n",
      "squirt.v\n",
      "shower.v\n",
      "drizzle.v\n",
      "heap.v\n",
      "pile.v\n",
      "pump.v\n",
      "jam.v\n",
      "plant.v\n",
      "scatter.v\n",
      "butter.v\n",
      "asphalt.v\n",
      "surface.v\n",
      "tile.v\n",
      "wallpaper.v\n",
      "coat.v\n",
      "suffuse.v\n",
      "fill.v\n",
      "strew.v\n",
      "douse.v\n",
      "flood.v\n",
      "crowd.v\n",
      "pave.v\n",
      "varnish.v\n",
      "paint.v\n",
      "gild.v\n",
      "glaze.v\n",
      "embellish.v\n",
      "panel.v\n",
      "wax.v\n",
      "wash.v\n",
      "plank.v\n",
      "yoke.v\n",
      "dress.v\n",
      "accessorize.v\n",
      "\n",
      "FRAME ELEMENTS:\n",
      "Agent\n",
      "Theme\n",
      "Source\n",
      "Path\n",
      "Goal\n",
      "Degree\n",
      "Depictive\n",
      "Manner\n",
      "Means\n",
      "Result\n",
      "Explanation\n",
      "Time\n",
      "Place\n",
      "Purpose\n",
      "Instrument\n",
      "Subregion\n",
      "Cause\n"
     ]
    }
   ],
   "source": [
    "#### print some properties of a frame structure in NLTK\n",
    "\n",
    "print('ID', f.ID)\n",
    "print('FRAME:',f.name)\n",
    "print('DEFINITION', f.definition)\n",
    "print()\n",
    "print('LEXICAL UNITS:')\n",
    "for lu in f.lexUnit:\n",
    "    print(lu)\n",
    "print()\n",
    "print('FRAME ELEMENTS:')\n",
    "for fe in f.FE:\n",
    "    print(fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRAME RELATIONS:\n",
      "Container_focused_placing\n",
      "Cause_motion\n",
      "Distributed_position\n",
      "Placing\n",
      "Filling\n"
     ]
    }
   ],
   "source": [
    "print('FRAME RELATIONS:')\n",
    "for relation in f.frameRelations:\n",
    "   # print(relation.subFrameName)\n",
    "    print(relation.superFrameName)\n",
    "    #print(relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Getting frames for predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frames can be evoked by many different words and phrases. In the following example, the subject and object of *cause* are also events and actually more information than the main predicate:\n",
    "\n",
    "```Vaccination can cause autism```\n",
    "\n",
    "In this notebook, we are restricting ourselves to predicates however, as it is more complex to decide whether subjects and objects denote events as well. To find the predicates, we can rely on the syntactic parsing by spaCy as we did in the previous notebook.\n",
    "\n",
    "We repeat here for convenience the cells with our example sentence and the dependency tree rendering. We also re-use our function for obtaining event tuples from the dependency relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# depending on how you installed spaCy, the name of the model might be different\n",
    "nlp = spacy.load(name='en_core_web_sm') \n",
    "text = \"John makes the cake . He got sick . He went to bed .\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"703451e1e4d24a738a792552016b8d8d-0\" class=\"displacy\" width=\"1975\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">makes</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">cake .</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">got</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">sick .</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">went</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">bed .</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,179.0 L1103.0,167.0 1087.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-5\" stroke-width=\"2px\" d=\"M1295,177.0 C1295,89.5 1445.0,89.5 1445.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,179.0 L1287,167.0 1303,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-6\" stroke-width=\"2px\" d=\"M1470,177.0 C1470,89.5 1620.0,89.5 1620.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,179.0 L1628.0,167.0 1612.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-703451e1e4d24a738a792552016b8d8d-0-7\" stroke-width=\"2px\" d=\"M1645,177.0 C1645,89.5 1795.0,89.5 1795.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-703451e1e4d24a738a792552016b8d8d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,179.0 L1803.0,167.0 1787.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicate_subject_object(doc, rels={'nsubj', 'dobj', 'prep'}):\n",
    "    \"\"\"\n",
    "    extract predicates with:\n",
    "    -subject\n",
    "    -object\n",
    "    \n",
    "    :param spacy.tokens.doc.Doc doc: spaCy object after processing text\n",
    "    \n",
    "    :rtype: list \n",
    "    :return: list of tuples (predicate, subject, object)\n",
    "    \"\"\"\n",
    "    predicates = {}\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in rels:\n",
    "            \n",
    "            head = token.head\n",
    "            head_id = head.i\n",
    "            \n",
    "            if head_id not in predicates:\n",
    "                predicates[head_id] = dict()\n",
    "            \n",
    "            predicates[head_id][token.dep_] = token.lemma_\n",
    "    \n",
    "    output = []\n",
    "    for pred_token, pred_info in predicates.items():\n",
    "        one_row = (doc[pred_token].lemma_, \n",
    "                   pred_info.get('nsubj', None),\n",
    "                   pred_info.get('dobj', None)\n",
    "                  )\n",
    "        output.append(one_row)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we can process the text with spaCy and obtain the events, we can now make a simple script to iterate over de event tuples and obtain all the frames for each event word. The next cell does that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('make', 'John', 'cake')\n",
      "Number of frames: 27\n",
      "['Arriving', 'Behind_the_scenes', 'Body_decoration', 'Building', 'Causation', 'Cause_change', 'Communicate_categorization', 'Cooking_creation', 'Creating', 'Destroying', 'Earnings_and_losses', 'Fame', 'Historic_event', 'Intentionally_create', 'Leadership', 'Make_acquaintance', 'Making_arrangements', 'Manufacturing', 'People_by_vocation', 'Personal_success', 'Procreative_sex', 'Reparation', 'Self_motion', 'Sex', 'Theft', 'Type', 'Verification']\n",
      "('get', '-PRON-', None)\n",
      "Number of frames: 51\n",
      "['Abandonment', 'Accompaniment', 'Accoutrements', 'Activity_prepare', 'Activity_start', 'Aiming', 'Amalgamation', 'Arriving', 'Board_vehicle', 'Body_movement', 'Bringing', 'Building', 'Cause_to_amalgamate', 'Cause_to_wake', 'Clothing', 'Collaboration', 'Come_down_with', 'Come_together', 'Contacting', 'Cooking_creation', 'Disembarking', 'Dressing', 'Dynamism', 'Escaping', 'Evading', 'Food', 'Gathering_up', 'Getting', 'Getting_underway', 'Getting_up', 'Giving_birth', 'Gizmo', 'Grasp', 'Intentional_deception', 'Making_arrangements', 'Mental_property', 'Personal_relationship', 'Punctual_perception', 'Purpose', 'Remembering_experience', 'Remembering_information', 'Remembering_to_do', 'Resolve_problem', 'Revenge', 'Social_event', 'Success_or_failure', 'Successfully_communicate_message', 'Text_creation', 'Transition_to_state', 'Travel', 'Waking_up']\n",
      "('go', '-PRON-', None)\n",
      "Number of frames: 79\n",
      "['Accoutrements', 'Appellations', 'Attempt', 'Attending', 'Avoiding', 'Being_named', 'Buildings', 'Categorization', 'Ceasing_to_be', 'Chatting', 'Clothing', 'Color', 'Come_into_effect', 'Commerce_scenario', 'Compatibility', 'Containers', 'Deny_or_grant_permission', 'Desirability', 'Desirable_event', 'Discussion', 'Dynamism', 'Emotion_active', 'Emotion_directed', 'Endeavor_failure', 'Event', 'Exemplar', 'Expend_resource', 'Experiencer_obj', 'Expertise', 'Fire_going_out', 'Firing', 'Forgoing', 'Getting_triggered', 'Getting_underway', 'Going_back_on_a_commitment', 'Hair_configuration', 'Have_associated', 'Ingestion', 'Isolated_places', 'Leadership', 'Losing_it', 'Make_noise', 'Morality_evaluation', 'Motion', 'Natural_features', 'Organization', 'Out_of_existence', 'People_by_origin', 'Perception_active', 'Perception_body', 'Personal_relationship', 'Possession', 'Process_continue', 'Proportional_quantity', 'Purpose', 'Rejuvenation', 'Releasing', 'Removing', 'Required_event', 'Scrutiny', 'Self_motion', 'Sex', 'Size', 'Sociability', 'Social_event_individuals', 'Social_interaction_evaluation', 'Speak_on_topic', 'Stimulus_focus', 'Store', 'Subjective_influence', 'Subordinates_and_superiors', 'Successful_action', 'Time_vector', 'Transition_to_a_quality', 'Undergo_change', 'Undergoing', 'Usefulness', 'Waver_between_options', 'Wearing']\n"
     ]
    }
   ],
   "source": [
    "events = get_predicate_subject_object(doc)\n",
    "for event in events:\n",
    "    predicate=event[0]\n",
    "    print(event)\n",
    "    frames = fn.frames_by_lemma(predicate)\n",
    "    print('Number of frames:', len(frames))\n",
    "    frame_names=[]\n",
    "    for frame in frames:\n",
    "        frame_names.append(frame.name)\n",
    "    print(frame_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these predicates are very polysemous! Many of these frames are very general and a-specific. So which of these frames are most relevant for our sentences? In other words, which frames tell the story!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to choose the right frame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is no data on the frame distribution for words. We can therefore not compare the frames in our domain with the frames in other collections of text data. For example, we cannot apply a contrastive analysis using ```TF*IDF``` for our domain data set to learn which frames are more frequent than expected.\n",
    "\n",
    "We are going to follow a different procedure here and check how well it works. We assume that words that are more specific also tend to have only a single frame and that these frames are precise indications of what the domain is about. Therefore if we can collect the frames from the words with a single frame (monosemous), we get statistics on the domain specific frames for free.\n",
    "\n",
    "We are going to do this in the following steps:\n",
    "\n",
    "<ol>\n",
    "    <li>For all events in all documents, put the frames of monosemous predicates in a list\n",
    "    <li>We count how frequently these frames occur and store the counts for later usage. we call these frames the dominant frames of the collection.\n",
    "    <li>When we process the events from a document, we check for each predicate if it has frames that match these dominant frames.\n",
    "    <li>If the dominant frame scores above a threshold, we keep the frame and the events, otherwise we ignore them.\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Learning the dominant frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our idea, we are going to apply this to a single document. We get all the event predicates and store the frame only for monosemous words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['People_by_vocation', 'Fields', 'Stimulus_focus', 'Expensiveness', 'Opinion', 'Cardinal_numbers', 'Dead_or_alive', 'People_by_vocation', 'Competition', 'Fluidic_motion', 'Fields', 'Opinion', 'Commerce_buy', 'Cause_change_of_position_on_a_scale', 'Gizmo', 'Alternatives', 'Judgment', 'Discussion', 'Emphasizing', 'Warning', 'Cause_to_be_wet', 'Commerce_buy']\n"
     ]
    }
   ],
   "source": [
    "#### Change the path to your own text file\n",
    "path_to_file='../lab1-getting-text/techcrunch_search_results/apple%20os%20x17.txt'\n",
    "monosemous_frame_counts = [] ### where we store the frames\n",
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "    doc = nlp(text)\n",
    "    events = get_predicate_subject_object(doc)\n",
    "    for event in events:\n",
    "        predicate=event[0]\n",
    "        frames = fn.frames_by_lemma(predicate)\n",
    "        if len(frames)==1:\n",
    "            monosemous_frame_counts.append(frames[0].name)\n",
    "\n",
    "\n",
    "print(monosemous_frame_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a list for a specific domain, we need to apply this to all texts in our collection. We load all the files from our folder to obtain the monosemous frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple%20os%20x17.txt\n",
      "apple%20os%20x16.txt\n",
      "apple%20os%20x14.txt\n",
      "apple%20os%20x9.txt\n",
      "apple%20os%20x28.txt\n",
      "apple%20os%20x29.txt\n",
      "apple%20os%20x8.txt\n",
      "apple%20os%20x15.txt\n",
      "apple%20os%20x39.txt\n",
      "apple%20os%20x11.txt\n",
      "apple%20os%20x10.txt\n",
      "apple%20os%20x38.txt\n",
      "apple%20os%20x12.txt\n",
      "apple%20os%20x13.txt\n",
      "apple%20os%20x60.txt\n",
      "apple%20os%20x48.txt\n",
      "apple%20os%20x49.txt\n",
      "apple%20os%20x61.txt\n",
      "apple%20os%20x59.txt\n",
      "apple%20os%20x58.txt\n",
      "apple%20os%20x55.txt\n",
      "apple%20os%20x41.txt\n",
      "apple%20os%20x40.txt\n",
      "apple%20os%20x54.txt\n",
      "apple%20os%20x42.txt\n",
      "apple%20os%20x56.txt\n",
      "apple%20os%20x57.txt\n",
      "apple%20os%20x43.txt\n",
      "apple%20os%20x47.txt\n",
      "apple%20os%20x53.txt\n",
      "apple%20os%20x52.txt\n",
      "apple%20os%20x46.txt\n",
      "apple%20os%20x50.txt\n",
      "apple%20os%20x44.txt\n",
      "apple%20os%20x45.txt\n",
      "apple%20os%20x51.txt\n",
      "apple%20os%20x36.txt\n",
      "apple%20os%20x3.txt\n",
      "apple%20os%20x22.txt\n",
      "apple%20os%20x23.txt\n",
      "apple%20os%20x2.txt\n",
      "apple%20os%20x37.txt\n",
      "apple%20os%20x21.txt\n",
      "apple%20os%20x35.txt\n",
      "apple%20os%20x1.txt\n",
      "apple%20os%20x34.txt\n",
      "apple%20os%20x20.txt\n",
      "apple%20os%20x18.txt\n",
      "apple%20os%20x24.txt\n",
      "apple%20os%20x30.txt\n",
      "apple%20os%20x5.txt\n",
      "apple%20os%20x4.txt\n",
      "apple%20os%20x31.txt\n",
      "apple%20os%20x25.txt\n",
      "apple%20os%20x19.txt\n",
      "apple%20os%20x33.txt\n",
      "apple%20os%20x6.txt\n",
      "apple%20os%20x27.txt\n",
      "apple%20os%20x26.txt\n",
      "apple%20os%20x7.txt\n",
      "apple%20os%20x32.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "monosemous_frame_counts = []\n",
    "\n",
    "basepath = Path('../lab1-getting-text/techcrunch_search_results/')\n",
    "files_in_basepath = basepath.iterdir()\n",
    "for path_to_file in files_in_basepath:\n",
    "    if path_to_file.is_file():  # check of the item is not a subdirectory!!\n",
    "        print(path_to_file.name)\n",
    "        with open(path_to_file) as infile:\n",
    "            text = infile.read()\n",
    "            doc = nlp(text)\n",
    "            events = get_predicate_subject_object(doc)\n",
    "            ### We iterate ove the events for a document\n",
    "            for event in events:\n",
    "                predicate=event[0]\n",
    "                frames = fn.frames_by_lemma(predicate)\n",
    "                if len(frames)==1:\n",
    "                    monosemous_frame_counts.append(frames[0].name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now again use the Counter function to obtain frame frequencies when we apply this to our whole collection. This will give us a reduced list of more specific frames with frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Emphasizing': 41, 'Performers_and_roles': 36, 'Quantified_mass': 23, 'People_by_vocation': 22, 'Give_impression': 15, 'Destroying': 15, 'Receiving': 14, 'Aggregate': 12, 'Performing_arts': 12, 'Inclusion': 11, 'Alternatives': 9, 'Temporal_subregion': 9, 'Stimulus_focus': 8, 'Gizmo': 8, 'Communication_means': 8, 'Containers': 8, 'Rescuing': 8, 'Delivery': 8, 'Calendric_unit': 8, 'Discussion': 7, 'Preventing_or_letting': 7, 'Exemplar': 7, 'Cause_to_start': 7, 'Firefighting': 7, 'Cardinal_numbers': 6, 'Commerce_buy': 6, 'Participation': 6, 'Predicament': 6, 'Request': 6, 'Choosing': 6, 'Similarity': 6, 'Emotion_directed': 6, 'Precariousness': 6, 'Statement': 6, 'Erasing': 6, 'Sequence': 6, 'Attempt': 5, 'Commerce_sell': 5, 'Architectural_part': 5, 'Judgment_communication': 5, 'Representing': 4, 'Assessing': 4, 'Render_nonfunctional': 4, 'Work': 4, 'Sufficiency': 4, 'Importance': 4, 'Information': 4, 'Conduct': 4, 'Clothing': 4, 'Protecting': 4, 'Differentiation': 4, 'Idiosyncrasy': 4, 'Improvement_or_decline': 4, 'Size': 4, 'Being_at_risk': 4, 'Judgment_direct_address': 4, 'Cause_change_of_position_on_a_scale': 3, 'Adopt_selection': 3, 'Purpose': 3, 'Evaluative_comparison': 3, 'Duplication': 3, 'Project': 3, 'Arranging': 3, 'Likelihood': 3, 'Fields': 2, 'Opinion': 2, 'Desiring': 2, 'Food': 2, 'Animals': 2, 'Buildings': 2, 'Activity_pause': 2, 'Adjusting': 2, 'Cogitation': 2, 'Execute_plan': 2, 'Create_representation': 2, 'Suitability': 2, 'Accuracy': 2, 'Affirm_or_deny': 2, 'Range': 2, 'Motion_noise': 2, 'Accomplishment': 2, 'Complaining': 2, 'Identicality': 2, 'Mental_property': 2, 'Shapes': 2, 'Deciding': 2, 'Bringing': 2, 'Cause_to_perceive': 2, 'Rite': 2, 'Capability': 2, 'Part_piece': 2, 'Change_event_time': 2, 'Continued_state_of_affairs': 2, 'Sound_movement': 2, 'Expectation': 2, 'Rejuvenation': 2, 'Emotion_active': 2, 'Emptying': 2, 'Natural_features': 2, 'Scrutiny': 2, 'Usefulness': 2, 'Gesture': 2, 'Attention': 2, 'Contacting': 2, 'Borrowing': 2, 'Expensiveness': 1, 'Dead_or_alive': 1, 'Competition': 1, 'Fluidic_motion': 1, 'Judgment': 1, 'Warning': 1, 'Cause_to_be_wet': 1, 'Law': 1, 'Compliance': 1, 'Telling': 1, 'Records': 1, 'Experimentation': 1, 'Shopping': 1, 'Organization': 1, 'Non-gradable_proximity': 1, 'Publishing': 1, 'Self_motion': 1, 'Finish_competition': 1, 'Distinctiveness': 1, 'Accoutrements': 1, 'Dispersal': 1, 'Becoming_aware': 1, 'Billing': 1, 'Deny_or_grant_permission': 1, 'Desirability': 1, 'Certainty': 1, 'Using': 1, 'Medical_conditions': 1, 'Non-commutative_process': 1, 'Adjacency': 1, 'Ingredients': 1, 'Locale_by_use': 1, 'Encoding': 1, 'Contrary_circumstances': 1, 'Catastrophe': 1, 'Presence': 1, 'Surpassing': 1, 'Text': 1, 'Instance': 1, 'Supporting': 1, 'Cause_motion': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counted_frames = Counter(monosemous_frame_counts)\n",
    "print(counted_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Choosing the dominant frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the above frame set as a distributional resource and load it for future usage. We use the ```pickle``` module to store the result as *binary* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dominant-frame-counts.pickle', 'wb') as outputfile:\n",
    "    pickle.dump(counted_frames, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the file at any moment using the same ```pickle``` module. We use a different variable to load the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Emphasizing': 41, 'Performers_and_roles': 36, 'Quantified_mass': 23, 'People_by_vocation': 22, 'Give_impression': 15, 'Destroying': 15, 'Receiving': 14, 'Aggregate': 12, 'Performing_arts': 12, 'Inclusion': 11, 'Alternatives': 9, 'Temporal_subregion': 9, 'Stimulus_focus': 8, 'Gizmo': 8, 'Communication_means': 8, 'Containers': 8, 'Rescuing': 8, 'Delivery': 8, 'Calendric_unit': 8, 'Discussion': 7, 'Preventing_or_letting': 7, 'Exemplar': 7, 'Cause_to_start': 7, 'Firefighting': 7, 'Cardinal_numbers': 6, 'Commerce_buy': 6, 'Participation': 6, 'Predicament': 6, 'Request': 6, 'Choosing': 6, 'Similarity': 6, 'Emotion_directed': 6, 'Precariousness': 6, 'Statement': 6, 'Erasing': 6, 'Sequence': 6, 'Attempt': 5, 'Commerce_sell': 5, 'Architectural_part': 5, 'Judgment_communication': 5, 'Representing': 4, 'Assessing': 4, 'Render_nonfunctional': 4, 'Work': 4, 'Sufficiency': 4, 'Importance': 4, 'Information': 4, 'Conduct': 4, 'Clothing': 4, 'Protecting': 4, 'Differentiation': 4, 'Idiosyncrasy': 4, 'Improvement_or_decline': 4, 'Size': 4, 'Being_at_risk': 4, 'Judgment_direct_address': 4, 'Cause_change_of_position_on_a_scale': 3, 'Adopt_selection': 3, 'Purpose': 3, 'Evaluative_comparison': 3, 'Duplication': 3, 'Project': 3, 'Arranging': 3, 'Likelihood': 3, 'Fields': 2, 'Opinion': 2, 'Desiring': 2, 'Food': 2, 'Animals': 2, 'Buildings': 2, 'Activity_pause': 2, 'Adjusting': 2, 'Cogitation': 2, 'Execute_plan': 2, 'Create_representation': 2, 'Suitability': 2, 'Accuracy': 2, 'Affirm_or_deny': 2, 'Range': 2, 'Motion_noise': 2, 'Accomplishment': 2, 'Complaining': 2, 'Identicality': 2, 'Mental_property': 2, 'Shapes': 2, 'Deciding': 2, 'Bringing': 2, 'Cause_to_perceive': 2, 'Rite': 2, 'Capability': 2, 'Part_piece': 2, 'Change_event_time': 2, 'Continued_state_of_affairs': 2, 'Sound_movement': 2, 'Expectation': 2, 'Rejuvenation': 2, 'Emotion_active': 2, 'Emptying': 2, 'Natural_features': 2, 'Scrutiny': 2, 'Usefulness': 2, 'Gesture': 2, 'Attention': 2, 'Contacting': 2, 'Borrowing': 2, 'Expensiveness': 1, 'Dead_or_alive': 1, 'Competition': 1, 'Fluidic_motion': 1, 'Judgment': 1, 'Warning': 1, 'Cause_to_be_wet': 1, 'Law': 1, 'Compliance': 1, 'Telling': 1, 'Records': 1, 'Experimentation': 1, 'Shopping': 1, 'Organization': 1, 'Non-gradable_proximity': 1, 'Publishing': 1, 'Self_motion': 1, 'Finish_competition': 1, 'Distinctiveness': 1, 'Accoutrements': 1, 'Dispersal': 1, 'Becoming_aware': 1, 'Billing': 1, 'Deny_or_grant_permission': 1, 'Desirability': 1, 'Certainty': 1, 'Using': 1, 'Medical_conditions': 1, 'Non-commutative_process': 1, 'Adjacency': 1, 'Ingredients': 1, 'Locale_by_use': 1, 'Encoding': 1, 'Contrary_circumstances': 1, 'Catastrophe': 1, 'Presence': 1, 'Surpassing': 1, 'Text': 1, 'Instance': 1, 'Supporting': 1, 'Cause_motion': 1})\n"
     ]
    }
   ],
   "source": [
    "loaded_counts=()\n",
    "with open('dominant-frame-counts.pickle', 'rb') as inputfile:\n",
    "    loaded_counts=pickle.load(inputfile)\n",
    "\n",
    "print(loaded_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can process any set of documents, obtain all events and check each event for the most dominant frame if any. To find the most dominant frame, we look up the associated frames in *loaded_counts*. If multiple frames are present, we take the one with the highest frequency. If none is present, we ignore the event.\n",
    "\n",
    "In the next cell, we show how to do this for a single file. We first set a threshold to only keep frames and events if the frame scores above this threshold. In this way, we can tune the degree of dominance and restrict the frames and events.\n",
    "\n",
    "If the predicate has multiple frames (polysemous), we keep the frame with the highest count. If the predicate has only one frame, this is the highest scoring one.\n",
    "\n",
    "Next, we only store the frame and event is the score is above the threshold and dominant. We keep track of the number of ignored events, given the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring this event: ('spend', None, 'year')\n",
      "Ignoring this event: ('industry', None, None)\n",
      "Ignoring this event: ('continue', 'war', None)\n",
      "Ignoring this event: ('free', 'delivery', None)\n",
      "Ignoring this event: ('wipe', 'bug', None)\n",
      "Ignoring this event: ('remove', 'sweep', 'rating')\n",
      "Ignoring this event: ('sweep', None, None)\n",
      "Ignoring this event: ('brand', None, None)\n",
      "Ignoring this event: ('free', None, None)\n",
      "Ignoring this event: ('Ahead', None, None)\n",
      "Ignoring this event: ('launch', None, None)\n",
      "Ignoring this event: ('affordable', 'service', None)\n",
      "Ignoring this event: ('launch', 'Spotify', 'app')\n",
      "Ignoring this event: ('subscriber', None, 'Oct')\n",
      "Ignoring this event: ('boost', None, 'subscription')\n",
      "Ignoring this event: ('launch', None, None)\n",
      "Ignoring this event: ('listen', None, None)\n",
      "Ignoring this event: ('onli', 'both', None)\n",
      "Ignoring this event: ('14.99', None, None)\n",
      "Ignoring this event: ('pricing', None, None)\n",
      "Ignoring this event: ('along', None, None)\n",
      "Ignoring this event: ('share', None, 'detail')\n",
      "Ignoring this event: ('14.99', None, None)\n",
      "Ignoring this event: ('establish', 'Walmart', 'accelerator')\n",
      "Ignoring this event: ('expansion', None, None)\n",
      "Ignoring this event: ('relationship', None, None)\n",
      "Ignoring this event: ('continue', 'which', None)\n",
      "Ignoring this event: ('shut', None, 'Vue')\n",
      "Ignoring this event: ('shut', 'service', None)\n",
      "Ignoring this event: ('come', 'news', None)\n",
      "Ignoring this event: ('launch', None, 'program')\n",
      "Ignoring this event: ('offering', None, '%')\n",
      "Ignoring this event: ('Perez', None, None)\n",
      "Ignoring this event: ('competition', None, None)\n",
      "Ignoring this event: ('earn', 'user', 'cash')\n",
      "Ignoring this event: ('Week', None, None)\n",
      "Ignoring this event: ('Welcome', None, None)\n",
      "Ignoring this event: ('recap', 'that', 'news')\n",
      "Ignoring this event: ('support', '-PRON-', None)\n",
      "Ignoring this event: ('flow', 'that', None)\n",
      "Ignoring this event: ('industry', None, None)\n",
      "Ignoring this event: ('5', None, None)\n",
      "Ignoring this event: ('introduce', 'Arcade', 'idea')\n",
      "Ignoring this event: ('idea', None, None)\n",
      "Ignoring this event: ('gaming', None, None)\n",
      "Ignoring this event: ('follow', 'Pass', None)\n",
      "Ignoring this event: ('subscribe', None, None)\n",
      "Ignoring this event: ('recommendation', None, None)\n",
      "Ignoring this event: ('think', 'service', None)\n",
      "Ignoring this event: ('base', None, None)\n",
      "Ignoring this event: ('come', 'app', None)\n",
      "Ignoring this event: ('Ahead', None, None)\n",
      "Ignoring this event: ('launch', None, None)\n",
      "Ignoring this event: ('platform', None, None)\n",
      "Ignoring this event: ('expand', 'Current', None)\n",
      "Ignoring this event: ('more', None, None)\n",
      "Ignoring this event: ('debut', 'Google', 'be')\n",
      "Ignoring this event: (\"'\", None, None)\n",
      "Ignoring this event: ('Perez', None, None)\n",
      "Ignoring this event: ('introduction', None, None)\n",
      "Ignoring this event: ('expand', 'company', 'feature')\n",
      "Ignoring this event: ('%', None, None)\n",
      "Ignoring this event: ('adult', None, None)\n",
      "Ignoring this event: ('%', None, None)\n",
      "Ignoring this event: ('disapprove', '-PRON-', None)\n",
      "Ignoring this event: ('majority', None, None)\n",
      "Ignoring this event: ('extend', 'that', None)\n",
      "Ignoring this event: ('accord', None, None)\n",
      "Ignoring this event: ('launch', 'Quibi', None)\n",
      "Ignoring this event: ('release', 'TikTok', 'set')\n",
      "Ignoring this event: ('', None, None)\n",
      "Ignoring this event: ('filter', None, 'comment')\n",
      "Ignoring this event: ('expand', 'Amazon', 'service')\n",
      "Ignoring this event: ('launch', 'that', None)\n",
      "Ignoring this event: ('expand', 'location', None)\n",
      "Ignoring this event: ('location', None, None)\n",
      "Ignoring this event: ('warn', 'Somorjai', None)\n",
      "Ignoring this event: ('dampen', 'politic', 'potential')\n",
      "Ignoring this event: ('rise', 'Carolina', None)\n",
      "Ignoring this event: ('s', '-PRON-', None)\n",
      "Ignoring this event: ('Load', None, 'more')\n",
      "Threshold= 2  ignored: 81  events out of: 168\n"
     ]
    }
   ],
   "source": [
    "### dictionary in which we store for each frame the list of events\n",
    "frame_event_dictionary={}\n",
    "\n",
    "### threshold how dominant the frame has to be to keep an event\n",
    "threshold=2\n",
    "\n",
    "#### Change the path to your own text file\n",
    "path_to_file='../lab1-getting-text/techcrunch_search_results/apple%20os%20x17.txt'\n",
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "    doc = nlp(text)\n",
    "    events = get_predicate_subject_object(doc)\n",
    "    number_of_ignored_events=0 ### counter to keep track of ignored frames\n",
    "    for event in events:\n",
    "        predicate=event[0] ## first element from the tuple\n",
    "        frames = fn.frames_by_lemma(predicate)\n",
    "        \n",
    "        ### best candidate frame\n",
    "        top_score=0\n",
    "        top_frame = \"\"\n",
    "        for frame in frames:\n",
    "            ### we get the count\n",
    "            count = loaded_counts[frame.name]\n",
    "            ### if it is higher than the current top score, we update the top score and the top frame\n",
    "            if (count>top_score):\n",
    "                top_score=count\n",
    "                top_frame=frame.name\n",
    "\n",
    "        ### if the top_score is above the threshold, we store the frame and the event\n",
    "        if top_score>threshold:\n",
    "            if top_frame in frame_event_dictionary:\n",
    "                ### if the frame is in the dictionary, we append the event to the list\n",
    "                frame_event_dictionary[top_frame].append(event)\n",
    "            else:\n",
    "                ### if the frame is not present, we create a new list entry for the frame with the event\n",
    "                frame_event_dictionary[top_frame]=[event]\n",
    "        else:\n",
    "            ## We ignore this event because it does not have a frame in our loaded_counts\n",
    "            print('Ignoring this event:',event)\n",
    "            number_of_ignored_events+=1\n",
    "\n",
    "print('Threshold=', threshold, ' ignored:', number_of_ignored_events, ' events out of:', len(events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line tells us how many events have been ignored because their frames are not dominant enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work [('work', 'Sarah', None), ('work', None, None), ('work', 'Sarah', None), ('work', 'that', None)]\n",
      "\n",
      "People_by_vocation [('writer', None, None), ('make', 'Amazon', None), ('serve', None, None), ('manager', None, None), ('report', None, None), ('teach', 'set', 'user'), ('home', None, None)]\n",
      "\n",
      "Emphasizing [('prior', None, None), ('as', None, None), ('grow', '-PRON-', None), ('m', None, None), ('focus', 's', None)]\n",
      "\n",
      "Quantified_mass [('number', None, None), ('bil', '194', None), ('let', 'ga', None), ('number', None, None)]\n",
      "\n",
      "Containers [('offer', 'late', 'pickup'), ('offer', 'service', 'booze'), ('back', None, None), ('back', None, None), ('back', None, None), ('offer', 'GameClub', 'hit'), ('offer', None, 'account')]\n",
      "\n",
      "Destroying [('late', None, None), ('take', 'GameClub', 'Oct'), ('out', None, None), ('out', None, None)]\n",
      "\n",
      "Judgment_communication [('tout', 'Walmart', None), ('raise', 'Current', 'B'), ('raise', None, 'billion')]\n",
      "\n",
      "Statement [('announce', 'retailer', 'milestone'), ('announce', 'company', 'deal'), ('aim', None, None), ('announce', 'Spotify', 'launch'), ('announce', None, 'pricing'), ('detail', None, None), ('announce', 'Walmart', 'expansion'), ('announce', 'company', None), ('announce', 'Venmo', 'program'), ('say', 'company', None), ('announce', 'company', None), ('announce', 'company', None), ('reac', 'service', None)]\n",
      "\n",
      "Stimulus_focus [('rating', None, None), ('rest', None, None), ('roll', 'Google', None), ('add', 'Spotify', 'feature'), ('roll', None, None), ('top', None, 'user')]\n",
      "\n",
      "Inclusion [('include', None, None), ('include', None, None), ('include', None, 'option')]\n",
      "\n",
      "Performers_and_roles [('app', None, None), ('be', 'service', None)]\n",
      "\n",
      "Preventing_or_letting [('allow', 'which', None), ('allow', 'which', None)]\n",
      "\n",
      "Cardinal_numbers [('three', 'child', None)]\n",
      "\n",
      "Clothing [('cost', 'Max', '14.99'), ('cost', 'service', '14.99')]\n",
      "\n",
      "Calendric_unit [('day', None, None)]\n",
      "\n",
      "Cause_to_start [('bring', 'Google', 'domain'), ('create', None, 'document'), ('create', 'number', 'majority')]\n",
      "\n",
      "Exemplar [('type', '-PRON-', 'doc.new')]\n",
      "\n",
      "Give_impression [('see', 'industry', None)]\n",
      "\n",
      "Aggregate [('collection', None, None), ('pop', 'that', 'recommendation'), ('combination', None, None), ('host', None, 'more'), ('set', None, None), ('set', None, None)]\n",
      "\n",
      "Commerce_buy [('buy', 'artist', \"'\"), ('buy', 'IBM', 'Hat')]\n",
      "\n",
      "Cause_change_of_position_on_a_scale [('promote', None, 'album')]\n",
      "\n",
      "Similarity [('like', '-PRON-', None)]\n",
      "\n",
      "Temporal_subregion [('begin', 'app', None), ('begin', 'which', None)]\n",
      "\n",
      "Gizmo [('tool', None, None)]\n",
      "\n",
      "Alternatives [('option', None, None)]\n",
      "\n",
      "Discussion [('discussion', None, None)]\n",
      "\n",
      "Commerce_sell [('sell', 'Quibi', None), ('sell', 's', None)]\n",
      "\n",
      "Information [('inform', None, 'user')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for frame, events in frame_event_dictionary.items():\n",
    "    print(frame, events)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the threshold above and see what types of frames and events get selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

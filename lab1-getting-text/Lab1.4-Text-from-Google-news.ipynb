{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1.4: Google news as a source of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the Google news API to search for news: \n",
    "\n",
    "https://news.google.com/?hl=en-US&gl=US&ceid=US:en\n",
    "\n",
    "We follow a similar approach as for Techcrunch and use BeautifulSoup to get the content. \n",
    "\n",
    "However, now we will also work with the structure of the Google news output. We do the usual imports for making a request and process the output through BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Making a request to the Goggle News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.google.com/?q=vaccines&hl=vaccines-en&gl=en\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "query='vaccines'\n",
    "\n",
    "query = query.lower()\n",
    "language='en'\n",
    "region='us'\n",
    "base_url = \"http://news.google.com\"\n",
    "query_url = \"{0}/?q={1}&hl={1}-{2}&gl={2}\".format(base_url, query, language, region)\n",
    "print(query_url)\n",
    "google_news_content= requests.get(query_url).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an HTML string. We can inspect the start of this string. Let's look at the first 1000 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html lang=\"en\" dir=\"ltr\"><head><base href=\"https://news.google.com/\"><meta name=\"referrer\" content=\"origin\"><link rel=\"canonical\" href=\"https://news.google.com/search\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,minimal-ui\"><meta name=\"apple-itunes-app\" content=\"app-id=459182288\"><meta name=\"google-site-verification\" content=\"AcBy5YFny2HQgVUCR18tO5YUTf6MpVlcJqGTd-a9-SI\"><meta name=\"mobile-web-app-capable\" content=\"yes\"><meta name=\"apple-mobile-web-app-capable\" content=\"yes\"><meta name=\"application-name\" content=\"News\"><meta name=\"apple-mobile-web-app-title\" content=\"News\"><meta name=\"apple-mobile-web-app-status-bar-style\" content=\"black\"><meta name=\"theme-color\" content=\"white\"><meta name=\"msapplication-tap-highlight\" content=\"no\"><link rel=\"shortcut icon\" href=\"https://lh3.googleusercontent.com/-DR60l-K8vnyi99NZovm9HlXyZwQ85GMDxiwJWzoasZYCUrPuUM_P_4Rb7ei03j-0nRs0c4F=w16\" sizes=\"16x16\"><link rel=\"shortcut icon\" href=\"https://lh3.googleusercontent.c'\n"
     ]
    }
   ],
   "source": [
    "print(google_news_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using BeatifulSoup to process the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use BeautifulSoup again to flesh out the results. So let's first turn the string returned by Google into a `soup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(google_news_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going into the specific structure of the Google news result structure but we define two specific functions that get the *author* of the news article and the actual *content*.\n",
    "\n",
    "Analyse the two functions and try to understand the code. Note that if the format of the Google news output changes, also the code may have to be adapted. These functions use regular expressions: https://docs.python.org/3/library/re.html Regular expressions allow you to 'parse' strings using simple patterns. We need to import the re package to use regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# This function requires the Beautiful soup object \n",
    "# that we get back from the request to the Google API\n",
    "\n",
    "def parse_author(html):\n",
    "    result = \"\"\n",
    "    html = BeautifulSoup(html, 'html.parser')\n",
    "    # Try Parsing Author from Meta Tags\n",
    "    author = html.find('meta', attrs={'name': re.compile('author')}) or \\\n",
    "             html.find('meta', property=re.compile('author', re.IGNORECASE))\n",
    "\n",
    "    if author: result = author['content']\n",
    "    else:  # Otherwise, try parsing Author from Text\n",
    "        author = html.find(attrs={'itemprop': 'author'}) or \\\n",
    "                 html.find(attrs={'class': 'byline'})\n",
    "        if author: result = author.text\n",
    "\n",
    "    return re.sub(r\"\\s+\", \" \", re.sub(\"by \", \"\", result, flags=re.IGNORECASE)).strip()\n",
    "\n",
    "\n",
    "# This function requires the HTML content we get \n",
    "# back from the request to the Google API\n",
    "\n",
    "def parse_news_text(html):\n",
    "\n",
    "    html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Try to find Article Body by Semantic Tag\n",
    "    article = html.find('article')\n",
    "\n",
    "    # Otherwise, try to find Article Body by Class Name (with the largest number of paragraphs)\n",
    "    if not article:\n",
    "        articles = html.find_all(class_=re.compile('(body|article|main)', re.IGNORECASE))\n",
    "        if articles:\n",
    "            article = sorted(articles, key=lambda x: len(x.find_all('p')), reverse=True)[0]\n",
    "\n",
    "    # Parse text from all Paragraphs\n",
    "    text = []\n",
    "    if article:\n",
    "        for paragraph in [tag.text for tag in article.find_all('p')]:\n",
    "            if re.findall(\"[.,!?]\", paragraph):\n",
    "                text.append(paragraph)\n",
    "    text = re.sub(r\"\\s+\", \" \", \" \".join(text))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google news output consists of articles. We therefore need code to iterate over all article data elements and collect the data. The next code shows how we aggregate the content and meta information for each article, where we apply the above functions to obtain the author and the text from the content obtained from the URL. We break the loop after the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.wbaltv.com/article/vaccine-exemption-rates-among-us-kindergartners-continue-to-climb-cdc-says-1/29522407', '20191019T131900', 'en', '', 'WBAL Baltimore', 'Vaccine exemption rates among US kindergartners continue to climb, CDC says', '']\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all Articles in Google News\n",
    "articles = soup.find_all('article')\n",
    "for i, article in enumerate(articles, 1):    \n",
    "    div, title, publisher = article.find_all('a')\n",
    "    time = re.sub(\"[Z\\-:]\", \"\", article.find('time').get('datetime'))\n",
    "\n",
    "    article_redirect = \"{}{}\".format(base_url, title.get('href')[1:])\n",
    "    article_url = requests.get(article_redirect).url\n",
    "    \n",
    "    news_content= requests.get(article_url).content\n",
    "    author = parse_author(news_content)\n",
    "    news_text = parse_news_text(news_content)\n",
    "    \n",
    "    # new_entry is going to contain the data for each article returned\n",
    "    new_entry = [article_url, \n",
    "                 time, \n",
    "                 language, \n",
    "                 author, \n",
    "                 publisher.text, \n",
    "                 title.text, \n",
    "                 news_text]\n",
    "    print(new_entry)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Save the search results in a CSV file with the meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to store the results for the complete loop in a CSV file so that we can load it later on. To create the output as CSV data, we are going to use the `Pandas` package: https://pandas.pydata.org\n",
    "Please follow the instructions to install pandas locally:\n",
    "\n",
    "* `conda install pandas`\n",
    "* `python -m pip install --upgrade pandas`\n",
    "\n",
    "Consult the documentation to learn more about the functionalities. Here we are going to use it to convert our list of featurures for a tweet to a CSV format.\n",
    "\n",
    "We need to import *os* for writing to a file and *pandas* (after the install) for dealing with the data structure. Take your time to study the next bit of code so that you understand the individual steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the columns for the result table. The data need to be stored in the order of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['url', \n",
    "        'created_at', \n",
    "        'lang', \n",
    "        'author',  \n",
    "        'publisher', \n",
    "        'title', \n",
    "        'news_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the same article processing for-loop to obtain the data for each article and store the result in a pandas data frame that we call `all_news_dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# We first define a data frame that we name 'all_news_dataframe' \n",
    "# with pandas imported as 'pd' using the columns list that we defined before.\n",
    "# Basically, we tell pandas what data will be stored.\n",
    "\n",
    "data=[]\n",
    "\n",
    "# Iterate over all Articles in Google News\n",
    "articles = soup.find_all('article')\n",
    "for i, article in enumerate(articles, 1):\n",
    "    try:\n",
    "        div, title, publisher = article.find_all('a')\n",
    "        time = re.sub(\"[Z\\-:]\", \"\", article.find('time').get('datetime'))\n",
    "\n",
    "        article_redirect = \"{}{}\".format(base_url, title.get('href')[1:])\n",
    "        ### Since the request call may generate an error from the website it tries to reach\n",
    "        ### We have to catch the error message so that we can continue with the next URL to obtain a result\n",
    "        ### To handle the errors, we create a try and except block. IF there is an error, we print it, otherwise we carry out commands\n",
    "        article_url = requests.get(article_redirect).url\n",
    "        #article_hash = int(abs(hash(article_url)))\n",
    "\n",
    "        news_content= requests.get(article_url).content\n",
    "        author = parse_author(news_content)\n",
    "        news_text = parse_news_text(news_content)\n",
    "\n",
    "        # new_entry is going to contain the data for each article returned\n",
    "        new_entry = [article_url, \n",
    "                     time, \n",
    "                     language, \n",
    "                     author, \n",
    "                     publisher.text, \n",
    "                     title.text, \n",
    "                     news_text]\n",
    "\n",
    "        data.append(new_entry)\n",
    "        # We now completed appending all the possible values for this tweet.\n",
    "        # We use the pandas framework imported as 'pd' to create a dataframe from the aggregated data in new_entry\n",
    "        # We need to provide the columns COLS to tell pandas what value belongs to what.\n",
    "        # Note that the data need to be aggregated in the same order as the names in COLS, otherwise values will get mixed up\n",
    "        #single_article_dataframe = pd.DataFrame([new_entry], columns=COLS)\n",
    "\n",
    "        # single_tweet_dataframe now contains the data for a single tweet\n",
    "        # next we add it to the data frame for all tweets 'all_tweets_dataframe'\n",
    "        # check the pandas documentation if you want to know what ignore_index=True does to the data aggregation\n",
    "        #all_news_dataframe = all_news_dataframe.append(single_article_dataframe, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "all_news_dataframe = pd.DataFrame(data, columns=COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the code, you may see a whole batch of error messages. Most likely, these are due to websites that cannot be reached. Basically, we fail to collect the data from the site. Still, some of the sites could be reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data frame basically is a table with columns and rows. We use the `shape` function to ask for the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7)\n"
     ]
    }
   ],
   "source": [
    "print(all_news_dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the *pandas* framework, we can now save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a file path to store the results as CSV. Make sure the folder 'googlenews_search_results' exists \n",
    "# or that you specify another path to an existing location. The 'news_results_<query>.csv' file will be created in that location.\n",
    "csvFilePath='googlenews_search_results/news_results_%s.csv' % query\n",
    "\n",
    "# we now open the csvFile for appending our result\n",
    "csvFile = open(csvFilePath,\"w+\")       \n",
    "all_news_dataframe.to_csv(csvFile, columns=COLS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the Google News API is no longer maintained. It is still running but it is not known for how long:\n",
    "\n",
    "https://medium.com/rakuten-rapidapi/top-10-best-news-apis-google-news-bloomberg-bing-news-and-more-bbf3e6e46af6\n",
    "\n",
    "\n",
    "In the next notebook Lab1.5, we show how you can access other news sources directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1.4: Google news as a source of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the Google news API to search for news: \n",
    "\n",
    "https://news.google.com/?hl=en-US&gl=US&ceid=US:en\n",
    "\n",
    "We follow a similar approach as for Techcrunch and use BeautifulSoup to get the content. However, now we have to deal with the structure of the Google news output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.google.com/?q=vaccins&hl=vaccins-en&gl=en\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "query='vaccins'\n",
    "\n",
    "query = query.lower()\n",
    "language='en'\n",
    "region='us'\n",
    "base_url = \"http://news.google.com\"\n",
    "query_url = \"{0}/?q={1}&hl={1}-{2}&gl={2}\".format(base_url, query, language, region)\n",
    "print(query_url)\n",
    "google_news_content= requests.get(query_url).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a complex HTML string. We can inspect the start of this string. Let's look at the first 500 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html lang=\"en\" dir=\"ltr\"><head><base href=\"https://news.google.com/\"><meta name=\"referrer\" content=\"origin\"><link rel=\"canonical\" href=\"https://news.google.com/search\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,minimal-ui\"><meta name=\"apple-itunes-app\" content=\"app-id=459182288\"><meta name=\"google-site-verification\" content=\"AcBy5YFny2HQgVUCR18tO5YUTf6MpVlcJqGTd-a9-SI\"><meta name=\"mobile-web-app-capable\" content=\"yes\"><meta name=\"apple-mobile-web-app-capabl'\n"
     ]
    }
   ],
   "source": [
    "print(google_news_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use BeautifulSoup again to flesh out the results. So let's first turn the string returned by Google into a *soup* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(google_news_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going into the specific structure of the Google news result structure but we define two specific functions that get the *author* of the news article and the actual *content*.\n",
    "\n",
    "Analyse the two functions and try to understand the code. Note that if the format of the Google news output changes, also the code may have to be adapted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function requires the Beautifule soup object  we get back from the request to the Google API\n",
    "\n",
    "def parse_author(html):\n",
    "    result = \"\"\n",
    "    html = BeautifulSoup(html, 'html.parser')\n",
    "    # Try Parsing Author from Meta Tags\n",
    "    author = html.find('meta', attrs={'name': re.compile('author')}) or \\\n",
    "             html.find('meta', property=re.compile('author', re.IGNORECASE))\n",
    "\n",
    "    if author: result = author['content']\n",
    "    else:  # Otherwise, try parsing Author from Text\n",
    "        author = html.find(attrs={'itemprop': 'author'}) or \\\n",
    "                 html.find(attrs={'class': 'byline'})\n",
    "        if author: result = author.text\n",
    "\n",
    "    return re.sub(r\"\\s+\", \" \", re.sub(\"by \", \"\", result, flags=re.IGNORECASE)).strip()\n",
    "\n",
    "\n",
    "#This function requires the HTML content we get back from the request to the Google API\n",
    "\n",
    "def parse_news_text(html):\n",
    "\n",
    "    html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Try to find Article Body by Semantic Tag\n",
    "    article = html.find('article')\n",
    "\n",
    "    # Otherwise, try to find Article Body by Class Name (with the largest number of paragraphs)\n",
    "    if not article:\n",
    "        articles = html.find_all(class_=re.compile('(body|article|main)', re.IGNORECASE))\n",
    "        if articles:\n",
    "            article = sorted(articles, key=lambda x: len(x.find_all('p')), reverse=True)[0]\n",
    "\n",
    "    # Parse text from all Paragraphs\n",
    "    text = []\n",
    "    if article:\n",
    "        for paragraph in [tag.text for tag in article.find_all('p')]:\n",
    "            if re.findall(\"[.,!?]\", paragraph):\n",
    "                text.append(paragraph)\n",
    "    text = re.sub(r\"\\s+\", \" \", \" \".join(text))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google news output consists of articles. We therefore need code to iterate over all article data elements and collect the data. The next code shows how we aggregate the content and meta information for each article, where we apply the above functions to obtain the author and the text from the content obtained from the URL. We break the loop after the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://newjersey.news12.com/story/41192477/paterson-officials-warn-some-residents-to-get-hepatitis-a-vaccine', '20191016T230500', 'en', '', 'News 12 New Jersey', 'Paterson officials warn some residents to get hepatitis A vaccine', \"Paterson officials are warning some of the city’s residents to get the hepatitis A vaccine if they shopped at a certain grocery store in the city. A worker at Brother’s Produce tested positive for the illness. Anyone who shopped at the store between Sept. 30 and Oct. 5 is urged to get the vaccine. City officials will be giving out free vaccines to anyone who does not have health insurance. Over 300 people received the vaccines on Wednesday. The Paterson Health Department is giving them out on a first-come, first-served basis. Looking to check out brewery this summer? There are many breweries across the state for beer-lovers to visit. Check out our list for breweries in your area. Looking to check out brewery this summer? There are many breweries across the state for beer-lovers to visit. Check out our list for breweries in your area. News 12 New Jersey wants to announce your wedding. If you were married in the past 90 days, please submit your photos via Twitter or Instagram using #JustMarriedNJ. News 12 New Jersey wants to announce your wedding. If you were married in the past 90 days, please submit your photos via Twitter or Instagram using #JustMarriedNJ. Craving something savory or sweet? It doesn't really matter when it comes to food trucks, where you can have it all -- you just have to know where to find them! Here is a list of food trucks and where you could find them in New Jersey. Craving something savory or sweet? It doesn't really matter when it comes to food trucks, where you can have it all -- you just have to know where to find them! Here is a list of food trucks and where you could find them in New Jersey.\"]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all Articles in Google News\n",
    "articles = soup.find_all('article')\n",
    "for i, article in enumerate(articles, 1):\n",
    "    ## new_entry is going to contain the data for each article returned\n",
    "    new_entry = []\n",
    "    \n",
    "    div, title, publisher = article.find_all('a')\n",
    "    time = re.sub(\"[Z\\-:]\", \"\", article.find('time').get('datetime'))\n",
    "\n",
    "    article_redirect = \"{}{}\".format(base_url, title.get('href')[1:])\n",
    "    article_url = requests.get(article_redirect).url\n",
    "    article_hash = int(abs(hash(article_url)))\n",
    "    \n",
    "    news_content= requests.get(article_url).content\n",
    "    author = parse_author(news_content)\n",
    "    news_text = parse_news_text(news_content)\n",
    "    \n",
    "    #new entry append in the order of the data frame\n",
    "    new_entry += [article_url, time, language, author, publisher.text, title.text, news_text]\n",
    "    print(new_entry)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the search results in a CSV file with the meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to store the results for the complete loop in a CSV file so that we can load it later on. To create the output as CSV data, we are going to use the Pandas package: https://pandas.pydata.org\n",
    "Please follow the instructions to install pandas locally:\n",
    "\n",
    "* conda install pandas\n",
    "* python -m pip install --upgrade pandas\n",
    "\n",
    "Consult the documentation to learn more about the functionalities. Here we are going to use it to convert our list of featurures for a tweet to a CSV format.\n",
    "\n",
    "We need to import *os* for writing to a file and *pandas* (after the install) for dealing with the data structure. Take your time to study the next bit of code so that you understand the individual steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the columns for the result table. The data need to be stored in the order of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['url', 'created_at', 'lang', 'author',  'publisher', 'title', 'news_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the same article processing for-loop to obtain the data for each article and store the result in a pandas data frame that we call *all_news_dataframe*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# We first define a data frame that we name 'all_news_dataframe' \n",
    "# with pandas imported as 'pd' using the columns list that we defined before.\n",
    "# Basically, we tell pandas what data will be stored.\n",
    "\n",
    "all_news_dataframe = pd.DataFrame(columns=COLS)\n",
    "\n",
    "# Iterate over all Articles in Google News\n",
    "articles = soup.find_all('article')\n",
    "for i, article in enumerate(articles, 1):\n",
    "    ## new_entry is going to contain the data for each article returned\n",
    "    new_entry = []\n",
    "    \n",
    "    div, title, publisher = article.find_all('a')\n",
    "    time = re.sub(\"[Z\\-:]\", \"\", article.find('time').get('datetime'))\n",
    "\n",
    "    article_redirect = \"{}{}\".format(base_url, title.get('href')[1:])\n",
    "    article_url = requests.get(article_redirect).url\n",
    "    article_hash = int(abs(hash(article_url)))\n",
    "    \n",
    "    news_content= requests.get(article_url).content\n",
    "    author = parse_author(news_content)\n",
    "    news_text = parse_news_text(news_content)\n",
    "    \n",
    "    #new entry append in the order of the data frame\n",
    "    new_entry += [article_url, time, language, author, publisher.text, title.text, news_text]\n",
    "\n",
    "\n",
    "    # We now completed appending all the possible values for this tweet.\n",
    "    # We use the pandas framework imported as 'pd' to create a dataframe from the aggregated data in new_entry\n",
    "    # We need to provide the columns COLS to tell pandas what value belongs to what.\n",
    "    # Note that the data need to be aggregated in the same order as the names in COLS, otherwise values will get mixed up\n",
    "    single_article_dataframe = pd.DataFrame([new_entry], columns=COLS)\n",
    "        \n",
    "    # single_tweet_dataframe now contains the data for a single tweet\n",
    "    # next we add it to the data frame for all tweets 'all_tweets_dataframe'\n",
    "    # check the pandas documentation if you want to know what ignore_index=True does to the data aggregation\n",
    "    all_news_dataframe = all_news_dataframe.append(single_article_dataframe, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the code, you may see a whole batch of error messages. Most likely, these are due to websites that cannot be reached. Basically, we fail to collect the data from the site. Still, some of the sites could be reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data frame basically is a table with columns and rows. We use the *shape* function to ask for the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7)\n"
     ]
    }
   ],
   "source": [
    "print(all_news_dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the *pandas* framework, we can now save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a file path to store the results as CSV. Make sure the folder 'googlenews_search_results' exists \n",
    "# or that you specify another path to an existing location. The 'news_results_<query>.csv' file will be created in that location.\n",
    "csvFilePath='googlenews_search_results/news_results_'+query+'.csv'\n",
    "\n",
    "# we now open the csvFile for appending our result\n",
    "csvFile = open(csvFilePath,\"w+\")       \n",
    "all_news_dataframe.to_csv(csvFile, columns=COLS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the Google News API is no longer maintained. It is still running but it is not known for how long:\n",
    "\n",
    "https://medium.com/rakuten-rapidapi/top-10-best-news-apis-google-news-bloomberg-bing-news-and-more-bbf3e6e46af6\n",
    "\n",
    "\n",
    "In the next notebook Lab1.5, we show how you can access other news sources directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

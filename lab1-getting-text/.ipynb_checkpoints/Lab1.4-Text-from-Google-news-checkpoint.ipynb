{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1.4: Google news as a source of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "\n",
    "class Article(object):\n",
    "    def __init__(self, language, time, url, title, publisher, author, raw):\n",
    "        self.language = language\n",
    "        self.title = title\n",
    "        self.publisher = publisher\n",
    "        self.author = author\n",
    "        self.time = time\n",
    "        self.url = url\n",
    "        self.raw = raw\n",
    "\n",
    "        self.hash = str(abs(hash(self.url)))\n",
    "\n",
    "    @property\n",
    "    def complete(self):\n",
    "        return self.language and self.time and self.url\n",
    "\n",
    "    def to_naf(self):\n",
    "        xml_header = '<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\\n'\n",
    "\n",
    "        naf = ElementTree.Element(\"NAF\")\n",
    "        naf.set(\"version\", \"v3\")\n",
    "        naf.set(\"xml:lang\", self.language)\n",
    "\n",
    "        header = ElementTree.SubElement(naf, \"nafHeader\")\n",
    "\n",
    "        file_desc = ElementTree.SubElement(header, \"fileDesc\")\n",
    "        file_desc.set(\"creationtime\", self.time)\n",
    "\n",
    "        if self.publisher: file_desc.set(\"publisher\", self.publisher)\n",
    "        if self.author: file_desc.set(\"author\", self.author)\n",
    "        if self.title: file_desc.set(\"title\", self.title)\n",
    "\n",
    "        public = ElementTree.SubElement(header, \"public\")\n",
    "        public.set(\"uri\", self.url)\n",
    "\n",
    "        raw = ElementTree.SubElement(naf, \"raw\")\n",
    "\n",
    "        # Add CData via Comment Hack (Add as Comment, then remove Comment Tags in the End)\n",
    "        raw.append(ElementTree.Comment(\"<![CDATA[{}]]>\".format(self.raw)))\n",
    "\n",
    "        output = xml_header + ElementTree.tostring(naf)\n",
    "\n",
    "        # Remove Comment (See CData Hack)\n",
    "        return output.replace('<!--', \"\").replace(\"-->\", \"\")\n",
    "\n",
    "\n",
    "def parse_author(html):\n",
    "    html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    # Try Parsing Author from Meta Tags\n",
    "    author = html.find('meta', attrs={'name': re.compile('author')}) or \\\n",
    "             html.find('meta', property=re.compile('author', re.IGNORECASE))\n",
    "\n",
    "    if author: result = author['content']\n",
    "    else:  # Otherwise, try parsing Author from Text\n",
    "        author = html.find(attrs={'itemprop': 'author'}) or \\\n",
    "                 html.find(attrs={'class': 'byline'})\n",
    "        if author: result = author.text\n",
    "\n",
    "    return re.sub(r\"\\s+\", \" \", re.sub(\"by \", \"\", result, flags=re.IGNORECASE)).strip()\n",
    "\n",
    "\n",
    "def parse_news(html):\n",
    "    author = parse_author(html)\n",
    "\n",
    "    html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Try to find Article Body by Semantic Tag\n",
    "    article = html.find('article')\n",
    "\n",
    "    # Otherwise, try to find Article Body by Class Name (with the largest number of paragraphs)\n",
    "    if not article:\n",
    "        articles = html.find_all(class_=re.compile('(body|article|main)', re.IGNORECASE))\n",
    "        if articles:\n",
    "            article = sorted(articles, key=lambda x: len(x.find_all('p')), reverse=True)[0]\n",
    "\n",
    "    # Parse text from all Paragraphs\n",
    "    text = []\n",
    "    if article:\n",
    "        for paragraph in [tag.text for tag in article.find_all('p')]:\n",
    "            if re.findall(\"[.,!?]\", paragraph):\n",
    "                text.append(paragraph)\n",
    "    text = re.sub(r\"\\s+\", \" \", \" \".join(text))\n",
    "\n",
    "    return author, text\n",
    "\n",
    "\n",
    "def get_news(query, language='en', region='us', cache=True):\n",
    "    \"\"\"\n",
    "    Get News Articles from Google News\n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        The Search Term\n",
    "    language: str\n",
    "        Search Language\n",
    "    region: str\n",
    "        Search Region\n",
    "    cache: bool\n",
    "        Use Cached Results (if Available)\n",
    "    \"\"\"\n",
    "\n",
    "    query = query.lower()\n",
    "\n",
    "    # Create Cache\n",
    "    cache_root = os.path.join('tmp', \"{}\".format(query), \"{}-{}\".format(language, region))\n",
    "    if not os.path.exists(cache_root):\n",
    "        os.makedirs(cache_root)\n",
    "\n",
    "    # If no cache exists or user wants to rebuild cache\n",
    "    if not cache or not len(os.listdir(cache_root)):\n",
    "\n",
    "        base_url = \"http://news.google.com\"\n",
    "        query_url = \"{0}/?q={1}&hl={1}-{2}&gl={2}\".format(base_url, query, language, region)\n",
    "\n",
    "        # Create Content Parser (Beautiful Soup)\n",
    "        soup = BeautifulSoup(requests.get(query_url).content, 'html.parser')\n",
    "\n",
    "        # Remove Existing Cache\n",
    "        for item in os.listdir(cache_root):\n",
    "            os.remove(os.path.join(cache_root, item))\n",
    "\n",
    "        # Iterate over all Articles in Google News\n",
    "        articles = soup.find_all('article')\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            div, title, publisher = article.find_all('a')\n",
    "\n",
    "            time = re.sub(\"[Z\\-:]\", \"\", article.find('time').get('datetime'))\n",
    "\n",
    "            article_redirect = \"{}{}\".format(base_url, title.get('href')[1:])\n",
    "            article_url = requests.get(article_redirect).url\n",
    "            article_hash = int(abs(hash(article_url)))\n",
    "\n",
    "            print(\"\\r[{:3d}/{:3d}] Downloading {}\".format(i, len(articles), article_url), end=\"\")\n",
    "\n",
    "            # Write Json Metadata to Cache\n",
    "            with open(os.path.join(cache_root, \"{}.json\".format(article_hash)), 'w') as json_file:\n",
    "                json.dump({'title': title.text, 'publisher': publisher.text, 'time': time, 'url': article_url}, json_file)\n",
    "\n",
    "            # Write HTML to Cache\n",
    "            with open(os.path.join(cache_root, \"{}.html\".format(article_hash)), 'w') as html:\n",
    "                html.write(requests.get(article_url).content)\n",
    "\n",
    "    json_files = [item for item in os.listdir(cache_root) if item.endswith('json')]\n",
    "    for i, path in enumerate(json_files, 1):\n",
    "\n",
    "        full_path_json = os.path.join(cache_root, path)\n",
    "        full_path_html = full_path_json.replace('.json', '.html')\n",
    "\n",
    "        with open(full_path_json) as json_file, open(full_path_html) as html_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "            print(\"\\r[{:3d}/{:3d}] Parsing {}\".format(i, len(json_files), data['url']), end=\"\")\n",
    "\n",
    "            yield Article(language, data['time'], data['url'], data['title'], data['publisher'], *parse_news(html_file.read()))\n",
    "\n",
    "\n",
    "def news_to_naf(articles, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for article in articles:\n",
    "        if article.complete:\n",
    "            xml_path = os.path.join(path, \"{}.xml\".format(article.hash))\n",
    "            with io.open(xml_path, 'w', encoding='utf-8') as xml_file:\n",
    "                xml_file.write(article.to_naf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1.5 New York Times news API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York Times provides a search API to access various news sources:\n",
    "\n",
    "https://developer.nytimes.com\n",
    "\n",
    "In this notebook, we are going to see how their API library can be used to obtain news for your keyword search, how you can set filters and how to obtain meta data and the text to save it.\n",
    "\n",
    "Credits:\n",
    "We thank the blog from Rochelle Terman from which we re-used some code and explanation:\n",
    "https://dlab.berkeley.edu/blog/scraping-new-york-times-articles-python-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations for using the NYT news API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access the nes stream and the archives, you first need to install the package *nytimesarticle* locally on your computer on command line:\n",
    "\n",
    "https://pypi.org/project/nytimesarticle/0.1.0/\n",
    "\n",
    "'pip install nytimesarticle==0.1.0'\n",
    "\n",
    "*nytimesarticle* is a python wrapper for the New York Times Article Search API. This allows you to query the API through python. \n",
    "\n",
    "With the Article Search API, you can search New York Times articles from Sept. 18, 1851 to today, retrieving headlines, abstracts, lead paragraphs, links to associated multimedia and other article metadata.\n",
    "\n",
    "The API will not return full text of articles. But it will return a number of helpful metadata such as subject terms, abstract, and date, as well as URLs, which one could conceivably use to scrape the full text of articles.\n",
    "\n",
    "To begin, you first need to obtain an API key from the New York Times, which is fast and easy to do: https://developer.nytimes.com/get-started\n",
    "\n",
    "* create an account\n",
    "* login\n",
    "* create app\n",
    "* get the id and key\n",
    "\n",
    "Once you have the key you can start using the API. Below is the code to create an API client that we call *api*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "\n",
    "app='LanguageAsData'\n",
    "appId='f00c2182-1fc4-4483-9f2c-cd577b9cd79f'\n",
    "appKey='i3OGS5ARPH8WxHxjuGmVWHbxEHxfy4dO'\n",
    "api = articleAPI(appKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the *api.search()* function to launch our query. It takes various parameters of which we illustrate a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"vaccination\"\n",
    "beginDate=20151231\n",
    "endDate=20181231\n",
    "filter={'source':['Reuters','AP', 'The New York Times']}\n",
    "filter={'source':['Reuters','AP']}\n",
    "\n",
    "\n",
    "articles = api.search(q = query,\n",
    "                      fq = str(filter), \n",
    "                      begin_date = beginDate, \n",
    "                      end_date= endDate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The q (for query) parameter searches the article's body, headline and byline for a particular term. In this case, we are looking for the search term ‘vaccins’. The fq (for filter query) parameter filters search results by various dimensions. For instance, 'source':['Reuters','The New York Times'] will filter by source (Reuters, New York Times, and AP are available through the API.) The begin_date and end_date parameter (in YYYYMMDD format) limits the date range of the search.\n",
    "\n",
    "As you can see, we can specify multiple filters by using a python dictionary and multiple values by using a list: fq = {'source':['Reuters','AP', 'The New York Times']}\n",
    "\n",
    "There are many other parameters and filters we can use to specify our serach. Get a full list here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the search is a dictionary that has the following keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fault'])\n"
     ]
    }
   ],
   "source": [
    "print(articles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fault': {'faultstring': 'Invalid ApiKey for given resource',\n",
       "  'detail': {'errorcode': 'oauth.v2.InvalidApiKeyForGivenResource'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the 'response', which contains a dictionary with 'docs' with the actual content as a list. The next command show the first element from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-99a88961fb4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'response'"
     ]
    }
   ],
   "source": [
    "print(articles['response']['docs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search function returns a dictionary of the first 10 results. To get the next 10, we have to use the page parameter. page = 2 returns the second 10 results, page = 3 the third 10 and so on.\n",
    "\n",
    "If you run the code, you'll see that the returned dictionary is pretty messy. What we’d really like to have is a list of dictionaries, with each dictionary representing an article and each dictionary representing a field of metadata from that article (e.g. headline, date, etc.) We can do this with a custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    # we create a list structure to capture the results\n",
    "    news = []\n",
    "    for doc in articles['response']['docs']:\n",
    "        # we define a dictionary to store all meta data and the text\n",
    "        dic = {}\n",
    "        dic['id'] = doc['_id']\n",
    "        if doc['abstract'] is not None:\n",
    "            dic['abstract'] = doc['abstract'].encode(\"utf8\")\n",
    "        dic['headline'] = doc['headline']['main'].encode(\"utf8\")\n",
    "        dic['desk'] = doc['news_desk']\n",
    "        dic['date'] = doc['pub_date'][0:10] # cutting time of day.\n",
    "        if doc['snippet'] is not None:\n",
    "            dic['snippet'] = doc['snippet'].encode(\"utf8\")\n",
    "        dic['source'] = doc['source']\n",
    "        dic['url'] = doc['web_url']\n",
    "        # locations\n",
    "        locations = []\n",
    "        for x in range(0,len(doc['keywords'])):\n",
    "            if 'glocations' in doc['keywords'][x]['name']:\n",
    "                locations.append(doc['keywords'][x]['value'])\n",
    "        dic['locations'] = locations\n",
    "        # subject\n",
    "        subjects = []\n",
    "        for x in range(0,len(doc['keywords'])):\n",
    "            if 'subject' in doc['keywords'][x]['name']:\n",
    "                subjects.append(doc['keywords'][x]['value'])\n",
    "        dic['subjects'] = subjects   \n",
    "        news.append(dic)\n",
    "    return(news) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function *parse_articles* to process the articles retrieved through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=parse_articles(articles)\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same approach as for the Google news API to store the news with the meta data in a CSV file, using the pandas framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['id', 'abstract', 'headline', 'desk','date',  'snippet', 'source', 'url', 'locations', 'subjects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# We first define a data frame that we name 'all_news_dataframe' \n",
    "# with pandas imported as 'pd' using the columns list that we defined before.\n",
    "# Basically, we tell pandas what data will be stored.\n",
    "\n",
    "all_news_dataframe = pd.DataFrame(columns=COLS)\n",
    "\n",
    "# Iterate over all news items\n",
    "for i, new_entry in enumerate(news, 1):\n",
    "\n",
    "    # We now completed appending all the possible values for this tweet.\n",
    "    # We use the pandas framework imported as 'pd' to create a dataframe from the aggregated data in new_entry\n",
    "    # We need to provide the columns COLS to tell pandas what value belongs to what.\n",
    "    # Note that the data need to be aggregated in the same order as the names in COLS, otherwise values will get mixed up\n",
    "    single_article_dataframe = pd.DataFrame([new_entry], columns=COLS)\n",
    "        \n",
    "    # single_tweet_dataframe now contains the data for a single tweet\n",
    "    # next we add it to the data frame for all tweets 'all_tweets_dataframe'\n",
    "    # check the pandas documentation if you want to know what ignore_index=True does to the data aggregation\n",
    "    all_news_dataframe = all_news_dataframe.append(single_article_dataframe, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_news_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a file path to store the results as CSV. Make sure the folder 'googlenews_search_results' exists \n",
    "# or that you specify another path to an existing location. The 'news_results_<query>.csv' file will be created in that location.\n",
    "csvFilePath='nyt_search_results/news_results_'+query+'.csv'\n",
    "\n",
    "# we now open the csvFile for appending our result\n",
    "csvFile = open(csvFilePath,\"w+\")       \n",
    "all_news_dataframe.to_csv(csvFile, columns=COLS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

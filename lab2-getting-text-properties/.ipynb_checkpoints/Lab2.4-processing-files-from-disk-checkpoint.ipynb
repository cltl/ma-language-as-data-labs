{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2.4 How to apply NLP to documents on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created three types of output when collecting data in the notebooks of Lab1:\n",
    "\n",
    "* text files with a document in each file\n",
    "* CSV files with a text and meta data in a row\n",
    "* JSON files with a text and meta data\n",
    "\n",
    "In order to obtain the statistics, you need to read all the text files one by one and apply your processing, you need to read a CSV file and process the rows one by one or load a JSON file and process the data units. We are going to learn how to do this next.\n",
    "\n",
    "In order to load data, it is good to know where you are running this notebook and what the path is to your data. For this we use the folowing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/piek/Desktop/Language-as-data/LAD-labs/lab2-getting-text-properties'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specifiy the path to our data and load it in our program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all text files from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple%20os%20x17.txt\n",
      "apple%20os%20x16.txt\n",
      "apple%20os%20x14.txt\n",
      "apple%20os%20x9.txt\n",
      "apple%20os%20x28.txt\n",
      "apple%20os%20x29.txt\n",
      "apple%20os%20x8.txt\n",
      "apple%20os%20x15.txt\n",
      "apple%20os%20x39.txt\n",
      "apple%20os%20x11.txt\n",
      "apple%20os%20x10.txt\n",
      "apple%20os%20x38.txt\n",
      "apple%20os%20x12.txt\n",
      "apple%20os%20x13.txt\n",
      "apple%20os%20x60.txt\n",
      "apple%20os%20x48.txt\n",
      "apple%20os%20x49.txt\n",
      "apple%20os%20x61.txt\n",
      "apple%20os%20x59.txt\n",
      "apple%20os%20x58.txt\n",
      "apple%20os%20x55.txt\n",
      "apple%20os%20x41.txt\n",
      "apple%20os%20x40.txt\n",
      "apple%20os%20x54.txt\n",
      "apple%20os%20x42.txt\n",
      "apple%20os%20x56.txt\n",
      "apple%20os%20x57.txt\n",
      "apple%20os%20x43.txt\n",
      "apple%20os%20x47.txt\n",
      "apple%20os%20x53.txt\n",
      "apple%20os%20x52.txt\n",
      "apple%20os%20x46.txt\n",
      "apple%20os%20x50.txt\n",
      "apple%20os%20x44.txt\n",
      "apple%20os%20x45.txt\n",
      "apple%20os%20x51.txt\n",
      "apple%20os%20x36.txt\n",
      "apple%20os%20x3.txt\n",
      "apple%20os%20x22.txt\n",
      "apple%20os%20x23.txt\n",
      "apple%20os%20x2.txt\n",
      "apple%20os%20x37.txt\n",
      "apple%20os%20x21.txt\n",
      "apple%20os%20x35.txt\n",
      "apple%20os%20x1.txt\n",
      "apple%20os%20x34.txt\n",
      "apple%20os%20x20.txt\n",
      "apple%20os%20x18.txt\n",
      "apple%20os%20x24.txt\n",
      "apple%20os%20x30.txt\n",
      "apple%20os%20x5.txt\n",
      "apple%20os%20x4.txt\n",
      "apple%20os%20x31.txt\n",
      "apple%20os%20x25.txt\n",
      "apple%20os%20x19.txt\n",
      "apple%20os%20x33.txt\n",
      "apple%20os%20x6.txt\n",
      "apple%20os%20x27.txt\n",
      "apple%20os%20x26.txt\n",
      "apple%20os%20x7.txt\n",
      "apple%20os%20x32.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# The path to the folder with the text files. \n",
    "# Here I use a relative path from where I run the notebook\n",
    "# Adapt the path accordingly to where your data is and/or where you run your notebook\n",
    "# You can also specify the absolute path\n",
    "\n",
    "basepath = Path('../lab1-getting-text/techcrunch_search_results/')\n",
    "files_in_basepath = basepath.iterdir()\n",
    "for item in files_in_basepath:\n",
    "    if item.is_file():  # check of the item is not a subdirectory!!\n",
    "        print(item.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read each file one by one and apply SpaCy processing to each. In the next loop, we iterate again over all the files, apply SpaCy NLP and print out the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple%20os%20x17.txt\n",
      "Nr. of sentences: 55\n",
      "apple%20os%20x16.txt\n",
      "Nr. of sentences: 39\n",
      "apple%20os%20x14.txt\n",
      "Nr. of sentences: 84\n",
      "apple%20os%20x9.txt\n",
      "Nr. of sentences: 21\n",
      "apple%20os%20x28.txt\n",
      "Nr. of sentences: 6\n",
      "apple%20os%20x29.txt\n",
      "Nr. of sentences: 6\n",
      "apple%20os%20x8.txt\n",
      "Nr. of sentences: 65\n",
      "apple%20os%20x15.txt\n",
      "Nr. of sentences: 232\n",
      "apple%20os%20x39.txt\n",
      "Nr. of sentences: 70\n",
      "apple%20os%20x11.txt\n",
      "Nr. of sentences: 65\n",
      "apple%20os%20x10.txt\n",
      "Nr. of sentences: 133\n",
      "apple%20os%20x38.txt\n",
      "Nr. of sentences: 44\n",
      "apple%20os%20x12.txt\n",
      "Nr. of sentences: 133\n",
      "apple%20os%20x13.txt\n",
      "Nr. of sentences: 232\n",
      "apple%20os%20x60.txt\n",
      "Nr. of sentences: 44\n",
      "apple%20os%20x48.txt\n",
      "Nr. of sentences: 66\n",
      "apple%20os%20x49.txt\n",
      "Nr. of sentences: 16\n",
      "apple%20os%20x61.txt\n",
      "Nr. of sentences: 11\n",
      "apple%20os%20x59.txt\n",
      "Nr. of sentences: 11\n",
      "apple%20os%20x58.txt\n",
      "Nr. of sentences: 20\n",
      "apple%20os%20x55.txt\n",
      "Nr. of sentences: 13\n",
      "apple%20os%20x41.txt\n",
      "Nr. of sentences: 26\n",
      "apple%20os%20x40.txt\n",
      "Nr. of sentences: 16\n",
      "apple%20os%20x54.txt\n",
      "Nr. of sentences: 44\n",
      "apple%20os%20x42.txt\n",
      "Nr. of sentences: 35\n",
      "apple%20os%20x56.txt\n",
      "Nr. of sentences: 20\n",
      "apple%20os%20x57.txt\n",
      "Nr. of sentences: 79\n",
      "apple%20os%20x43.txt\n",
      "Nr. of sentences: 26\n",
      "apple%20os%20x47.txt\n",
      "Nr. of sentences: 16\n",
      "apple%20os%20x53.txt\n",
      "Nr. of sentences: 13\n",
      "apple%20os%20x52.txt\n",
      "Nr. of sentences: 20\n",
      "apple%20os%20x46.txt\n",
      "Nr. of sentences: 30\n",
      "apple%20os%20x50.txt\n",
      "Nr. of sentences: 20\n",
      "apple%20os%20x44.txt\n",
      "Nr. of sentences: 30\n",
      "apple%20os%20x45.txt\n",
      "Nr. of sentences: 65\n",
      "apple%20os%20x51.txt\n",
      "Nr. of sentences: 44\n",
      "apple%20os%20x36.txt\n",
      "Nr. of sentences: 94\n",
      "apple%20os%20x3.txt\n",
      "Nr. of sentences: 17\n",
      "apple%20os%20x22.txt\n",
      "Nr. of sentences: 25\n",
      "apple%20os%20x23.txt\n",
      "Nr. of sentences: 70\n",
      "apple%20os%20x2.txt\n",
      "Nr. of sentences: 61\n",
      "apple%20os%20x37.txt\n",
      "Nr. of sentences: 16\n",
      "apple%20os%20x21.txt\n",
      "Nr. of sentences: 30\n",
      "apple%20os%20x35.txt\n",
      "Nr. of sentences: 65\n",
      "apple%20os%20x1.txt\n",
      "Nr. of sentences: 17\n",
      "apple%20os%20x34.txt\n",
      "Nr. of sentences: 94\n",
      "apple%20os%20x20.txt\n",
      "Nr. of sentences: 65\n",
      "apple%20os%20x18.txt\n",
      "Nr. of sentences: 39\n",
      "apple%20os%20x24.txt\n",
      "Nr. of sentences: 25\n",
      "apple%20os%20x30.txt\n",
      "Nr. of sentences: 2\n",
      "apple%20os%20x5.txt\n",
      "Nr. of sentences: 65\n",
      "apple%20os%20x4.txt\n",
      "Nr. of sentences: 18\n",
      "apple%20os%20x31.txt\n",
      "Nr. of sentences: 14\n",
      "apple%20os%20x25.txt\n",
      "Nr. of sentences: 16\n",
      "apple%20os%20x19.txt\n",
      "Nr. of sentences: 30\n",
      "apple%20os%20x33.txt\n",
      "Nr. of sentences: 14\n",
      "apple%20os%20x6.txt\n",
      "Nr. of sentences: 18\n",
      "apple%20os%20x27.txt\n",
      "Nr. of sentences: 16\n",
      "apple%20os%20x26.txt\n",
      "Nr. of sentences: 62\n",
      "apple%20os%20x7.txt\n",
      "Nr. of sentences: 21\n",
      "apple%20os%20x32.txt\n",
      "Nr. of sentences: 44\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en')\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "files_in_basepath = basepath.iterdir()\n",
    "for path_to_file in files_in_basepath:\n",
    "    if path_to_file.is_file():  # check of the item is not a subdirectory!!\n",
    "        print(path_to_file.name)\n",
    "        with open(path_to_file) as infile:\n",
    "            text = infile.read()\n",
    "            doc = nlp(text)\n",
    "            sents=list(doc.sents)\n",
    "            spacy_sentence=sents[0]\n",
    "            print(\"Nr. of sentences:\", len(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a CSV file using the pandas framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the pandas framework before to create data frames and save the data to a CSV file. We now are going to use the same framework for loading the CSV file and iterating over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache using fc-list. This may take a moment.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1184464377535254529</td>\n",
       "      <td>Wed Oct 16 13:41:28 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @and_kell: Do you know vaccines do indeed t...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>terriblewis1</td>\n",
       "      <td>autism</td>\n",
       "      <td>and_kell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1184161500031791104</td>\n",
       "      <td>Tue Oct 15 17:37:56 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Corporations will make money off your brain da...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>freedomgirl2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1184080523414638598</td>\n",
       "      <td>Tue Oct 15 12:16:10 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://andrewmmorris.com/\" rel=\"nofol...</td>\n",
       "      <td>RT @sallyKP: From 2011... listen.\\n\\nVery sad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>andrewmorrisuk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sallyKP</td>\n",
       "      <td>Bedford</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1184080355512459264</td>\n",
       "      <td>Tue Oct 15 12:15:30 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @sallyKP: From 2011... listen.\\n\\nVery sad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>anhisu7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sallyKP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1184075933118013440</td>\n",
       "      <td>Tue Oct 15 11:57:56 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @sallyKP: From 2011... listen.\\n\\nVery sad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>WarriorWifeMom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sallyKP</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                      created_at  \\\n",
       "0  1184464377535254529  Wed Oct 16 13:41:28 +0000 2019   \n",
       "1  1184161500031791104  Tue Oct 15 17:37:56 +0000 2019   \n",
       "2  1184080523414638598  Tue Oct 15 12:16:10 +0000 2019   \n",
       "3  1184080355512459264  Tue Oct 15 12:15:30 +0000 2019   \n",
       "4  1184075933118013440  Tue Oct 15 11:57:56 +0000 2019   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "2  <a href=\"http://andrewmmorris.com/\" rel=\"nofol...   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                          tweet_text lang  favorite_count  \\\n",
       "0  RT @and_kell: Do you know vaccines do indeed t...   en               0   \n",
       "1  Corporations will make money off your brain da...   en               1   \n",
       "2  RT @sallyKP: From 2011... listen.\\n\\nVery sad ...   en               0   \n",
       "3  RT @sallyKP: From 2011... listen.\\n\\nVery sad ...   en               0   \n",
       "4  RT @sallyKP: From 2011... listen.\\n\\nVery sad ...   en               0   \n",
       "\n",
       "   retweet_count  original_author hashtags user_mentions          place  \\\n",
       "0            350     terriblewis1   autism      and_kell            NaN   \n",
       "1              0  freedomgirl2011      NaN           NaN            NaN   \n",
       "2             38   andrewmorrisuk      NaN       sallyKP        Bedford   \n",
       "3             38          anhisu7      NaN       sallyKP            NaN   \n",
       "4             38   WarriorWifeMom      NaN       sallyKP  United States   \n",
       "\n",
       "   place_coord_boundaries  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Pandas libraries with alias 'pd' \n",
    "import pandas as pd \n",
    "\n",
    "# The path to the CSV file. Here I use a relative path from where I run the notebook\n",
    "# Adapt the path accordingly to where your data is and/or where you run your notebook\n",
    "# You can also specify the absolute path\n",
    "csvFilePath= '../lab1-getting-text/twitter_search_results/twitter_results_vaccination.csv'\n",
    "\n",
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "data = pd.read_csv(csvFilePath) \n",
    "# Preview the first 5 lines of the loaded data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate over the rows of the data and obtain the elements. In the next example, we just print the elements of the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 id                                                      1184464377535254529\n",
      "created_at                                   Wed Oct 16 13:41:28 +0000 2019\n",
      "source                    <a href=\"https://mobile.twitter.com\" rel=\"nofo...\n",
      "tweet_text                RT @and_kell: Do you know vaccines do indeed t...\n",
      "lang                                                                     en\n",
      "favorite_count                                                            0\n",
      "retweet_count                                                           350\n",
      "original_author                                                terriblewis1\n",
      "hashtags                                                             autism\n",
      "user_mentions                                                      and_kell\n",
      "place                                                                   NaN\n",
      "place_coord_boundaries                                                  NaN\n",
      "Name: 0, dtype: object\n",
      "The tweet text: RT @and_kell: Do you know vaccines do indeed trigger an encephalopathy in vulnerable children. Later diagnosed as #autism\n",
      "\n",
      "Doctors who push…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in data.iterrows(): \n",
    "    print(i, row) \n",
    "    print('The tweet text:', row['tweet_text'])\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the text from each tweet, we can create a loop to apply SpaCy to each and obtain information, as we did for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184464377535254529  nr of tokens: 26\n",
      "1184161500031791104  nr of tokens: 22\n",
      "1184080523414638598  nr of tokens: 33\n",
      "1184080355512459264  nr of tokens: 33\n",
      "1184075933118013440  nr of tokens: 33\n",
      "1183721999710990336  nr of tokens: 28\n",
      "1183721694294413312  nr of tokens: 26\n",
      "1183514755895611394  nr of tokens: 33\n",
      "1183496226651693056  nr of tokens: 26\n",
      "1183441061475160064  nr of tokens: 33\n",
      "1183347551162044416  nr of tokens: 33\n",
      "1183226084021678080  nr of tokens: 33\n",
      "1183221915650912257  nr of tokens: 33\n",
      "1183208475058413568  nr of tokens: 33\n",
      "1183169191802597376  nr of tokens: 33\n",
      "1183150143950020608  nr of tokens: 33\n",
      "1183144674506051585  nr of tokens: 33\n",
      "1183108700686278656  nr of tokens: 33\n",
      "1183045058364346369  nr of tokens: 33\n",
      "1183035899195478016  nr of tokens: 33\n",
      "1183019722583035906  nr of tokens: 26\n",
      "1183019522351083525  nr of tokens: 26\n",
      "1183016950735048704  nr of tokens: 33\n",
      "1183010825084248064  nr of tokens: 33\n",
      "1183007211724902400  nr of tokens: 33\n",
      "1183004312256929792  nr of tokens: 33\n",
      "1182997505497280515  nr of tokens: 33\n",
      "1182997121634578432  nr of tokens: 33\n",
      "1182996627495235586  nr of tokens: 27\n",
      "1182984888003248128  nr of tokens: 26\n",
      "1182832226804756483  nr of tokens: 30\n",
      "1182830860686581762  nr of tokens: 30\n",
      "1182830706428272641  nr of tokens: 30\n",
      "1182830336130125824  nr of tokens: 30\n",
      "1182828627739758592  nr of tokens: 25\n",
      "1182817623479898112  nr of tokens: 22\n",
      "1182713490684960770  nr of tokens: 30\n",
      "1182678839169814528  nr of tokens: 30\n",
      "1182677507356672001  nr of tokens: 30\n",
      "1182677370039234563  nr of tokens: 30\n",
      "1182657354577448960  nr of tokens: 25\n",
      "1182508355711111169  nr of tokens: 30\n",
      "1182453683990212608  nr of tokens: 30\n",
      "1182428049998077952  nr of tokens: 30\n",
      "1182334048183545856  nr of tokens: 30\n",
      "1182324981713326083  nr of tokens: 30\n",
      "1182318163108749313  nr of tokens: 30\n",
      "1182315216220098562  nr of tokens: 30\n",
      "1182312309139611648  nr of tokens: 30\n",
      "1182312041480118272  nr of tokens: 30\n",
      "1182311073585741824  nr of tokens: 30\n",
      "1182311006388854784  nr of tokens: 30\n",
      "1182310027652149248  nr of tokens: 30\n",
      "1182309854570041351  nr of tokens: 28\n",
      "1182246374924017665  nr of tokens: 28\n",
      "1182197044116623360  nr of tokens: 25\n",
      "1182175789225168896  nr of tokens: 24\n",
      "1181980158821097473  nr of tokens: 28\n",
      "1181960095686627329  nr of tokens: 28\n",
      "1181908521987837952  nr of tokens: 28\n",
      "1181896343238971392  nr of tokens: 28\n",
      "1181890146913968130  nr of tokens: 25\n",
      "1181882133742260225  nr of tokens: 28\n",
      "1181837092537565184  nr of tokens: 25\n",
      "1181773469098401793  nr of tokens: 28\n",
      "1181769439076737024  nr of tokens: 28\n",
      "1181769326807961600  nr of tokens: 28\n",
      "1181766248415162369  nr of tokens: 28\n",
      "1181765318869434368  nr of tokens: 26\n",
      "1181747911161462786  nr of tokens: 25\n",
      "1181747564518854657  nr of tokens: 25\n",
      "1181747506650259457  nr of tokens: 25\n",
      "1181746880310587394  nr of tokens: 25\n",
      "1181746095149522945  nr of tokens: 25\n",
      "1181680624966492160  nr of tokens: 25\n",
      "1181680497887461376  nr of tokens: 25\n",
      "1181679544455090177  nr of tokens: 25\n",
      "1181650132170874880  nr of tokens: 25\n",
      "1181623506263564294  nr of tokens: 25\n",
      "1181612372676485120  nr of tokens: 25\n",
      "1181611370875248640  nr of tokens: 25\n",
      "1181609887517806593  nr of tokens: 25\n",
      "1181606367708286977  nr of tokens: 25\n",
      "1181605762377764865  nr of tokens: 28\n",
      "1181602679111475200  nr of tokens: 28\n",
      "1181600984826630145  nr of tokens: 28\n",
      "1181600070522855425  nr of tokens: 28\n",
      "1181598543821365248  nr of tokens: 28\n",
      "1181597609720516608  nr of tokens: 28\n",
      "1181590160653389829  nr of tokens: 25\n",
      "1181581591564320768  nr of tokens: 25\n",
      "1181580125856550913  nr of tokens: 25\n",
      "1181561044008931328  nr of tokens: 25\n",
      "1181554993163378689  nr of tokens: 25\n",
      "1181554377204486145  nr of tokens: 25\n",
      "1181550120619450369  nr of tokens: 25\n",
      "1181546703662600193  nr of tokens: 25\n",
      "1181540033012084736  nr of tokens: 25\n",
      "1181539167131578368  nr of tokens: 25\n",
      "1181538330611085312  nr of tokens: 25\n",
      "1181536311192248320  nr of tokens: 25\n",
      "1181535933620994048  nr of tokens: 25\n",
      "1181532479854989313  nr of tokens: 28\n",
      "1181532260509605888  nr of tokens: 28\n",
      "1181527331476840448  nr of tokens: 28\n",
      "1181526054978166784  nr of tokens: 28\n",
      "1181506202041274369  nr of tokens: 25\n",
      "1181485594637152256  nr of tokens: 25\n",
      "1181447553436585985  nr of tokens: 25\n",
      "1181444497495871488  nr of tokens: 25\n",
      "1181437034478465027  nr of tokens: 25\n",
      "1181434882096029696  nr of tokens: 26\n",
      "1181417751497363457  nr of tokens: 25\n",
      "1181415460954411008  nr of tokens: 25\n",
      "1181408165059076098  nr of tokens: 25\n",
      "1181405817972486144  nr of tokens: 25\n",
      "1181401656614887426  nr of tokens: 25\n",
      "1181398194468610048  nr of tokens: 25\n",
      "1181396397930754048  nr of tokens: 25\n",
      "1181388750573703174  nr of tokens: 25\n",
      "1181386750016442368  nr of tokens: 25\n",
      "1181385424067018752  nr of tokens: 28\n",
      "1181384592835715072  nr of tokens: 25\n",
      "1181383730117664768  nr of tokens: 25\n",
      "1181383357382496257  nr of tokens: 25\n",
      "1181380697665232896  nr of tokens: 25\n",
      "1181379473230503936  nr of tokens: 25\n",
      "1181379036876066821  nr of tokens: 25\n",
      "1181376722341679104  nr of tokens: 18\n",
      "1181324109256495105  nr of tokens: 29\n",
      "1181302571954311168  nr of tokens: 24\n",
      "1181300653051637761  nr of tokens: 28\n",
      "1181280398497603584  nr of tokens: 25\n",
      "1181268166426664961  nr of tokens: 28\n",
      "1181268046339399680  nr of tokens: 28\n",
      "1181243316924403712  nr of tokens: 26\n",
      "1181241746937798658  nr of tokens: 28\n",
      "1181239831378190336  nr of tokens: 33\n",
      "1181230610649452545  nr of tokens: 26\n",
      "1181219872585506817  nr of tokens: 33\n",
      "1181219494947151873  nr of tokens: 33\n",
      "1181208446974267392  nr of tokens: 28\n",
      "1181146569002893312  nr of tokens: 29\n",
      "1181116156440002560  nr of tokens: 28\n",
      "1181102194088714240  nr of tokens: 29\n"
     ]
    }
   ],
   "source": [
    "for i, row in data.iterrows(): \n",
    "    id = row['id']\n",
    "    text = row['tweet_text']\n",
    "    doc = nlp(text)\n",
    "    sents=list(doc.sents)\n",
    "    nTokens=0\n",
    "    for token in doc:\n",
    "        nTokens += 1\n",
    "    print (id, ' nr of tokens:', nTokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rember that we also dumped the tweets as a JSON file. Next, we show how to read JSON and iterate over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jsonFilePath= '../lab1-getting-text/twitter_search_results/twitter_results_vaccination.json'\n",
    "\n",
    "\n",
    "# read file\n",
    "with open(jsonFilePath, 'r') as jsonfile:\n",
    "    data=jsonfile.read()\n",
    "\n",
    "# parse file\n",
    "json_data = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now loaded the data in the json_data object. Remember that we stored the tweets as json values for the tweet ids. We should therefore be able to iterate over the items json_data one-by-one and obtain the key (this is the id) and the value (the json structure for the tweet). We can then get the different data elements that we created before using the names for each element. Below, we show how this works for the first data element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1186154284947144704, 'Mon Oct 21 05:36:33 +0000 2019', '<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>', '@kwilks1120 No. Most diagnosed w #autism are born with it. There’s autism in kids who didn’t have #vaccines. Most d… https://t.co/RIIHqR5iZj', 'en', 2, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key,value in json_data.items():\n",
    "    new_entry = [value['id'], \n",
    "                      value['created_at'],\n",
    "                      value['source'], \n",
    "                      value['text'],\n",
    "                      value['lang'],\n",
    "                      value['favorite_count'], \n",
    "                      value['retweet_count']]\n",
    "    print(new_entry)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB3.5 Evaluating entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes evaluation for the task of entity linking. It covers the following aspects:\n",
    "1. Aggregating scores\n",
    "2. Evaluation procedure\n",
    "3. Example\n",
    "4. Example in code\n",
    "\n",
    "### 1. Aggregating scores\n",
    "\n",
    "The correctness of an entity linking system is measured in terms of precision, recall, and F1-score. \n",
    "\n",
    "Previously for the task of sentiment classification, we have seen that the precision, recall, and the F1-score were computed **per class**. This makes sense for tasks such as sentiment classification, because the number of classes is small and known in advance. For example, the classes in sentiment classification are: negative, neutral, and positive.\n",
    "\n",
    "If we apply the same logic to the task of entity linking, we get a huge number of classes. One class is for example the JFK airport, another one is the president JFK, another one is Paris in France, etc. The total number of classes is in the range of millions and changes when the knowledge base changes: for example, if Wikipedia adds 10,000 new entities tomorrow, the number of classes increases by 10,000. In addition, there is one special class in this evaluation: the class of NIL entities, which covers all entities for which there is no link in the knowledge base we use. \n",
    "\n",
    "While computing per-class F1-score is possible for entity linking too and is sometimes done, it is  more common to use a different procedure, namely to score our entity linking systems **per occurrence of an entity mention**. Likewise, we  aggregate the F1-score per entity mention occurrence in this course. We explain the procedure in more detail in part 2 of this notebook.\n",
    "\n",
    "### 2. Evaluation procedure\n",
    "Similar to evaluations of classes, we create a confusion matrix across system and gold performance but in this case for entity mentions. For each of the mentions in text, we compare the system decision against the gold data:\n",
    "* If the system chooses entity X and the gold entity is also X, and X is not a NIL entity, then we count a *true positive (TP)*\n",
    "* If the system chooses entity X, but the gold entity is Y, then we count a *false positive (FP)* and a *false negative (FN)*\n",
    "* If the system opts for a NIL entity and the gold entity is X, then we count a *false negative (FN)*\n",
    "* If the system opts for an entity X but the gold entity is NIL, then we count a *false positive (FP)*\n",
    "* If the system and the gold entities are both NIL, then we count a *true negative (TN)* - however, true negatives are not considered when computing the precision, recall, and F1-score.\n",
    "\n",
    "Afterwards, we use these numbers for TP, FP, and FN, to compute precision, recall, and F1-score in the usual way:\n",
    "\n",
    "* `precision=TP/(TP+FP)` -> From the decisions made by the system, how many were true\n",
    "* `recall=TP/(TP+FN)` -> From the gold entities, how many were found correctly by the system\n",
    "* `f1=2*precision*recall/(precision+recall)` -> compute a harmonic mean between precision and recall, called F1-score\n",
    "\n",
    "Note that precision, recall, and F1-score would all be the same in case all entities in the system output and the gold output are not NIL entities.\n",
    "\n",
    "### 3. Example\n",
    "\n",
    "For the example sentence we used in the introductory notebook:\n",
    "\n",
    "\"_JetBlue_ begins direct service between _Barnstable Airport_ and _JFK_.\"\n",
    "\n",
    "let's say that a system made the following decisions:\n",
    "* \"JetBlue\" means http://dbpedia.org/resource/JetBlue (true positive)\n",
    "* \"Barnstable Airport\" means http://dbpedia.org/resource/Barnstable,_Massachusetts (false positive)\n",
    "* \"JFK\" means http://dbpedia.org/resource/John_F._Kennedy (false positive and false negative)\n",
    "\n",
    "Then, we have in total: `TP=1, FP=2, FN=1`. \n",
    "\n",
    "The resulting precision is `1/3=0.33` and the resulting recall is `1/2=0.5`. \n",
    "\n",
    "The F1-score of this system on this sentence would be `0.40`. \n",
    "\n",
    "### 4. Example in code\n",
    "\n",
    "Now we provide a code for this scenario. Note that for simplicity we assume that the entity recognition by the system is perfect. Also, we use a simple format of the gold and the system output as a list, in practice this requires some more preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entity_linking(system_decisions, gold_decisions):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1-score by comparing two paired lists of: system decisions and gold data decisions.\n",
    "    \"\"\"\n",
    "    tp=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    \n",
    "    for gold_entity,system_entity in zip(gold_decisions,system_decisions):\n",
    "        if gold_entity=='NIL' and system_entity=='NIL': continue\n",
    "        if gold_entity==system_entity:\n",
    "            tp+=1\n",
    "        else:\n",
    "            if gold_entity!='NIL':\n",
    "                fn+=1\n",
    "            if system_entity!='NIL':\n",
    "                fp+=1\n",
    "\n",
    "    print('TP: %d; \\nFP: %d, \\nFN: %d' % (tp, fp, fn))            \n",
    "\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"JetBlue begins direct service between Barnstable Airport and JFK.\"\n",
    "\n",
    "gold_decisions=['http://dbpedia.org/resource/JetBlue', \n",
    "                'NIL',\n",
    "                'http://dbpedia.org/resource/John_F._Kennedy_International_Airport']\n",
    "system_decisions=['http://dbpedia.org/resource/JetBlue', \n",
    "                  'http://dbpedia.org/resource/Barnstable,_Massachusetts',\n",
    "                 'http://dbpedia.org/resource/John_F._Kennedy']\n",
    "\n",
    "precision, recall, f1 = evaluate_entity_linking(system_decisions, gold_decisions)\n",
    "\n",
    "print('Precision: %.2f, \\nRecall: %.2f, \\nF1-score: %.2f' % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

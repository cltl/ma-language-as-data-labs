{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB3.3 - Disambiguating with existing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many existing entity linking tools out there. We will not build our own, but instead run two existing tools and dig deeper into their output. In order to do this, we will do the following steps:\n",
    "1. Setup your environment (installation of modules and download of dataset)\n",
    "2. Load the dataset\n",
    "3. Run disambiguation with our tools on top of recognized mentions\n",
    "4. Run entity annotation from scratch: recognition and disambiguation together\n",
    "\n",
    "### 1. Setup your environment\n",
    "\n",
    "#### 1.1 Install the needed modules\n",
    "\n",
    "For the purpose of this week's coding exercises, we will need some new libraries that are probably not installed on your computer:\n",
    "* [rdflib](https://pypi.org/project/rdflib/)\n",
    "* (perhaps) [lxml](https://pypi.org/project/lxml/)\n",
    "\n",
    "You should install these libraries using `conda` or `pip`.\n",
    "\n",
    "Let's now check if all needed libraries are installed on your computer and can be imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "import json\n",
    "\n",
    "# import our own utility functions and classes\n",
    "import lab3_utils as utils\n",
    "import lab3_classes as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you see any errors with the imports:**\n",
    "* read the error carefully\n",
    "* install the library that is missing\n",
    "* try to execute the imports again\n",
    "\n",
    "Don't proceed before the imports work; we will use these modules in the explanation below and in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Obtain the N3 Reuters-128 dataset\n",
    "\n",
    "**This step should be done if you downloaded or cloned the course repository from GitHub.**\n",
    "\n",
    "We will work with a small dataset called Reuters-128. This dataset contains 128 Reuters documents annotated with entity mentions and links to DBpedia. You probably have this dataset if you downloaded or cloned the course repository from GitHub. Otherwise, you can download it from https://raw.githubusercontent.com/dice-group/n3-collection/master/Reuters-128.ttl.\n",
    "\n",
    "Store the dataset file 'Reuters-128.ttl' in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before proceding, please verify that your setup of libraries (1.1) is correct and that the dataset is downloaded in the right location (1.2).*\n",
    "\n",
    "Congratulations, you are all set up!\n",
    "\n",
    "### 2. Load the data from N3\n",
    "\n",
    "**Let's now parse this dataset to a list of news items objects that contain: 1) the text and 2) the entity mentions+links for each news item.**\n",
    "\n",
    "The data is in a .ttl format called NIF (don't worry about these formats for now, we provide a function to parse this dataset to python classes).\n",
    "You can open the .ttl file in Jupyter or a text editor for inspection. You will see records as the following, where the first represents a Reuters text and the second record represents an entity Dillon Read and Co that is mentioned in the the text:\n",
    "\n",
    "```\n",
    "<http://aksw.org/N3/Reuters-128/41#char=0,481>\n",
    "      a       nif:String , nif:Context , nif:RFC5147String ;\n",
    "      nif:beginIndex \"0\"^^xsd:nonNegativeInteger ;\n",
    "      nif:endIndex \"481\"^^xsd:nonNegativeInteger ;\n",
    "      nif:isString \"Avnet Inc said it filed with the Securities and Exchange Commission a registration statement for a proposed public offering of 150 mln dlrs of convertible subordinated debentures due 2012. Avnet said it will use the net proceeds for general working capital purposes and the anticipated domestic and foreign expansion of its distribution, assembly and manufacturing businesses. The company said an investment banking group managed by Dillon Read and Co Inc will handle the offering.\"@en ;\n",
    "      nif:sourceUrl <http://www.research.att.com/~lewis/Reuters-21578/15054> .\n",
    "      \n",
    "<http://aksw.org/N3/Reuters-128/41#char=433,455>\n",
    "      a       nif:RFC5147String ;\n",
    "      nif:anchorOf \"Dillon Read and Co Inc\"^^xsd:string ;\n",
    "      nif:beginIndex \"433\"^^xsd:nonNegativeInteger ;\n",
    "      nif:endIndex \"455\"^^xsd:nonNegativeInteger ;\n",
    "      nif:referenceContext\n",
    "              <http://aksw.org/N3/Reuters-128/41#char=0,481> ;\n",
    "      itsrdf:taIdentRef <http://dbpedia.org/resource/Dillon,_Read_%26_Co.> ;\n",
    "      itsrdf:taSource \"DBpedia_en_3.9\"^^xsd:string .\n",
    "```\n",
    "\n",
    "We provided the python code in *lab3_utils.py* and *lab3_classes.py* to process these NIF structures and to get the data. Make sure these files are in the same location as the notebook that you are running. If not, you should have seen an error with the import function above. If succesfully imported, you can now use the functions that are defined in these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_file='Reuters-128.ttl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=utils.load_article_from_nif_file(reuters_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data description**\n",
    "\n",
    "If you check the *lab3_utils.py* file, you will see that `articles` is a list of news articles (members of the `NewsItem` which we defined in the file *lab3_classes.py*). To help you understand, we now present the information that we store for each news article. \n",
    "\n",
    "Each news item contains the following fields: `identifier` (the original identifier of this document in the dataset), `dct` (the document creation time, if known), `content` (the text of this document), and `entity_mentions` (a list of entity mention occurrences that were found in this document).\n",
    "\n",
    "For each of the entity mentions, we store the following information: `sentence` (in which sentence was this mention found), `mention` (the exact phrase of this mention), `the_type` (its type, if known), `begin_index` (the starting offset of this mention), `end_index` (the ending offset of this mention), `gold_link` (the gold link for this mention), `aida_link` (the link for this mention proposed by AIDA), and `spotlight_link` (the link for this mention proposed by Spotlight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lab4_classes.NewsItem"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article=articles[0]\n",
    "type(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Disambiguation with existing entity linking tools\n",
    "\n",
    "We will perform entity linking on this dataset with two modern tools, called AIDA and DBpedia Spotlight.\n",
    "\n",
    "#### 3.1 AIDA\n",
    "\n",
    "AIDA is an entity linking system that puts all entity candidates in a graph network and then performs a probabilistic optimization to find the best connected candidate in this graph for each of the entity mentions. We call this **collective** or **global** disambiguation, because all entity mentions are disambiguated together.\n",
    "\n",
    "AIDA uses two kinds of connections in their algorithm: \n",
    "* The first kind is between a mention and an entity instance and tells us how often is a mention referred to by an instance. For example, the mention \"Jordan\" refers most often to the country, while less often to the basketball player, Michael Jordan. \n",
    "* The second kind of connections is between two entity instances; it tells us how well-connected are two entities. For example, the country Jordan is better connected with Saudi Arabia then it is with the basketball club San Antonio.\n",
    "\n",
    "You can play with AIDA by using their demo page: https://gate.d5.mpi-inf.mpg.de/webaida/.\n",
    "\n",
    "Here is a text you can try:\n",
    "```\n",
    "The President opened a window into the state of his mind Sunday when he lashed out against Jennifer Williams, an aide to Vice President Mike Pence, who described his July 25 call with Ukrainian President Volodymyr Zelensky in her deposition as \"inappropriate.\"\n",
    "```\n",
    "\n",
    "More description can be found in their paper: http://www.aclweb.org/anthology/D11-1072.\n",
    "\n",
    "In the function `aida_disambiguation` below, we do the following: \n",
    "* we iterate the 128 documents of the Reuters-128 dataset (called `articles`)\n",
    "* for each document, we combine the text and the marking of the entity mentions in a single string `new_content`\n",
    "* we send a request to AIDA to disambiguate the entities in this string\n",
    "* we store the entity links back to the `articles` in the field `aida_link`\n",
    "\n",
    "**Note:** You might encounter an SSL error such as `SSL: CERTIFICATE_VERIFY_FAILED` when running the urlopen command. This happens sometimes when your browser does not trust the certificate of some pages, it is a safety mechanism. Do the following to tell your browser that it can trust the web service we use:\n",
    "\n",
    "```import ssl\n",
    "context = ssl._create_unverified_context()\n",
    "```\n",
    "Then use this context in the urlopen call:\n",
    "\n",
    "`urllib.urlopen(\"https://your-url\", context=context)`\n",
    "\n",
    "If you can't fix this immediately, please ask us during the lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aida_disambiguation(articles, aida_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with AIDA.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:  #pbar provides a nice progress bar for the interation over the articles\n",
    "        for i, article in enumerate(articles):\n",
    "                                    \n",
    "            # AIDA expects entity mentions that are pre-marked inside text. \n",
    "            # For example, the sentence \"Obama visited Paris today\", \n",
    "            # should be transformed to \"[[Obama]] visited [[Paris]] today.\"\n",
    "            # We do this in the next 5 lines of code.\n",
    "            original_content = article.content\n",
    "            new_content=original_content\n",
    "            for entity in reversed(article.entity_mentions):\n",
    "                entity_span=new_content[entity.begin_index: entity.end_index]\n",
    "                new_content=new_content[:entity.begin_index] + '[[' + entity_span + ']]' + new_content[entity.end_index:]\n",
    "\n",
    "            # Now, we can run the AIDA library with this string.\n",
    "            params={\"text\": new_content, \"tag_mode\": 'manual'}\n",
    "            request = Request(aida_url, urlencode(params).encode())\n",
    "            this_json = urlopen(request).read().decode('unicode-escape')\n",
    "            try:\n",
    "                results=json.loads(this_json)\n",
    "            except:\n",
    "                continue\n",
    "            # Let's normalize the disambiguated entities.\n",
    "            # This means mostly removing the first part of the URI which is always the same (YAGO:)\n",
    "            # and leaving only the entity identification part (e.g., Barack_Obama).\n",
    "            dis_entities={}\n",
    "            for dis_entity in results['mentions']:\n",
    "                if 'bestEntity' in dis_entity.keys():\n",
    "                    best_entity=dis_entity['bestEntity']['kbIdentifier']\n",
    "                    clean_url=best_entity[5:] #SKIP YAGO:\n",
    "                else:\n",
    "                    clean_url='NIL'\n",
    "                dis_entities[str(dis_entity['offset'])] = clean_url # BECOMES THE VALUE IN THE DICTIONARY FOR THE OFFSET(REPRESENTING THE START OF THE MENTION) IN THE TEXT\n",
    "                \n",
    "            # We can now store the entity to our class instance for later processing.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                try:\n",
    "                    dis_url = str(dis_entities[str(start)])  # WE GET THE DISAMBIGUATED URL\n",
    "                except:\n",
    "                    dis_url='NIL'\n",
    "                entity.aida_link = dis_url  # THE ENTITY IS ENRICHED WITH THE AIDA_LINK\n",
    "\n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1:   1%|          | 1/128 [00:02<06:02,  2.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 128:  96%|█████████▌| 123/128 [06:40<00:16,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# AIDA is running in an external location - for this reason, we need to send an HTTP request. This should take a few minutes.\n",
    "aida_disambiguation_url = \"https://gate.d5.mpi-inf.mpg.de/aida/service/disambiguate\"\n",
    "\n",
    "processed_aida=aida_disambiguation(articles, aida_disambiguation_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The progress bar is sometimes stopping before 128. To be sure whether the command is done executing, please pay attention to the asterisk sign left from the cell. Once the '\\*' sign turns into a number, you can be sure that you can proceed with the notebook! Also, look at the first printed number (left from the progress bar), it should say “processed: 128” when it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_aida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIL\n",
      "NIL\n",
      "West_Germany\n",
      "McLarty–Watts_Ministry\n"
     ]
    }
   ],
   "source": [
    "# print the URLs for the entities in the articles, we break after the first\n",
    "for article in processed_aida:\n",
    "    for entity in article.entity_mentions:\n",
    "        print(entity.aida_link)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 DBpedia Spotlight\n",
    "\n",
    "DBpedia Spotlight (or only \"Spotlight\" for brevity) is an entity recognition and linking tool that performs linking to DBpedia. The core of their method is a vector space model to compute similarity between the text to annotate and the Wikipedia pages of all entity candidates for a mention. Then the entities with largest similarity are chosen.\n",
    "\n",
    "You can try out their demo as well: http://dbpedia-spotlight.github.com/demo/.\n",
    "\n",
    "Here is their paper: http://oa.upm.es/8923/1/DBpedia_Spotlight.pdf\n",
    "\n",
    "In the function `spotlight_disambiguation` below, we do the following: \n",
    "* we iterate the 128 documents of the Reuters-128 dataset (called `articles`)\n",
    "* for each document, we combine the text and the marking of the entity mentions in a single string `new_content`\n",
    "* we send a request to Spotlight to disambiguate the entities in this string\n",
    "* we store the entity links back to the `articles` in the field `spotlight_link`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DB Spotlight API expects XML as the input. Below is how the XML looks like for the first article:\n",
    "```\n",
    "<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?>\n",
    "<annotation text=\"West Germanys total net direct investments abroad fell to 11.2 billion marks in 1986 from 13.6 billion in 1985, but investments in developing countries rose to 683 mln marks from 358 mln, the Economics Ministry said. Foreign investments in West Germany were a net 5.8 billion marks in 1986, up from 3.6 billion in 1985, with higher European investments largely responsible for this increase. The ministry noted that, despite last years rise in West German investments in developing countries, the 1986 level was below the investments of over two billion marks seen in 1981, 1982 and 1983.\">\n",
    "<surfaceForm name=\"West Germanys\" offset=\"0\"/>\n",
    "<surfaceForm name=\"Economics Ministry\" offset=\"192\"/>\n",
    "<surfaceForm name=\"West Germany\" offset=\"240\"/>\n",
    "<surfaceForm name=\"The ministry\" offset=\"392\"/>\n",
    "</annotation>'\n",
    "```\n",
    "\n",
    "The XML consists of an ```<annoation>``` element with the text as an attribute and a list of surface forms and their offset position in the text to be annotated.\n",
    "To create this XML, we use the *lxml* package and specifically the *etree* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotlight_disambiguate(articles, spotlight_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with DBpedia Spotlight.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "            # Similar as with AIDA, we first prepare the document text and the mentions\n",
    "            # in order to provide these to Spotlight as input.\n",
    "            \n",
    "            # We build up the XML structure that Spotligh wants as input\n",
    "            # The next function Element creates the XML element with the text attribute\n",
    "            annotation = etree.Element(\"annotation\", text=article.content)\n",
    "            \n",
    "            # We iterate over the eneity mentions from our Reuters data to create the surface form elements\n",
    "            for mention in article.entity_mentions:\n",
    "                sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "                sf.set(\"name\", mention.mention)\n",
    "                sf.set(\"offset\", str(mention.begin_index))\n",
    "            my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "            # Send a disambiguation request to spotlight\n",
    "            results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                                  headers={'Accept': 'application/json'})\n",
    "            # Note that you can adjust the confidence value. Check the online demo to see the effect. \n",
    "            # What will happen with the recall and precision if you increase the confidence?\n",
    "            \n",
    "            # Process the results and normalize the entity URIs\n",
    "            j=results.json()\n",
    "            dis_entities={}\n",
    "            if 'Resources' in j: \n",
    "                resources=j['Resources']\n",
    "            else: \n",
    "                resources=[]\n",
    "            for dis_entity in resources:\n",
    "                dis_entities[str(dis_entity['@offset'])] = utils.normalizeURL(dis_entity['@URI'])\n",
    "            \n",
    "            # Let's now store the URLs by Spotlight to our class for later analysis.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                if str(start) in dis_entities:\n",
    "                    dis_url = dis_entities[str(start)]\n",
    "                else:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.spotlight_link = dis_url\n",
    "    \n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "                \n",
    "            # Pause for 100ms to prevent overloading the server\n",
    "            time.sleep(0.1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 128: 100%|██████████| 128/128 [02:05<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Spotlight is running in an external location - for this reason, we need to send an HTTP request. Hopefully, this will not take a long time.\n",
    "\n",
    "spotlight_disambiguation_url=\"http://model.dbpedia-spotlight.org/en/disambiguate\"\n",
    "\n",
    "processed_both=spotlight_disambiguate(processed_aida, spotlight_disambiguation_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Comparing the output of AIDA and Spotlight to the gold links\n",
    "\n",
    "Because we stored the decisions by both tools in our list `articles`, we can now compare their output.\n",
    "\n",
    "Let's pick an article and print the decisions made by AIDA and Spotlight on that article, and then compare that to the gold link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://aksw.org/N3/Reuters-128/10#char=0,488\n",
      "|mention: NV Philips Gloeilampenfabrieken\t|gold:\tPhilips\t|aida:\tNIL\t|spotlight:\tNIL |\n",
      "|mention: New York Stock Exchange\t|gold:\tNew_York_Stock_Exchange\t|aida:\tNew_York_Stock_Exchange\t|spotlight:\tNew_York_Stock_Exchange |\n",
      "|mention: Philips\t|gold:\tPhilips\t|aida:\tPhilips\t|spotlight:\tPhilips |\n",
      "|mention: Cor van der Klugt\t|gold:\thttp://de.dbpedia.org/resource/Cornelis_van_der_Klugt\t|aida:\tNIL\t|spotlight:\tNIL |\n",
      "|mention: Dutch\t|gold:\tDutch\t|aida:\tNetherlands\t|spotlight:\tDutch_language |\n",
      "|mention: NASDAQ\t|gold:\tNASDAQ\t|aida:\tNASDAQ\t|spotlight:\tNASDAQ |\n",
      "|mention: NYSE\t|gold:\tNew_York_Stock_Exchange\t|aida:\tNew_York_Stock_Exchange\t|spotlight:\tNew_York_Stock_Exchange |\n",
      "|mention: Philips\t|gold:\tPhilips\t|aida:\tPhilips\t|spotlight:\tPhilips |\n",
      "|mention: New York\t|gold:\tNew_York\t|aida:\tNew_York\t|spotlight:\tNew_York |\n",
      "|mention: U.S.\t|gold:\tUnited_States\t|aida:\tUnited_States\t|spotlight:\tUnited_States |\n",
      "|mention: Philips\t|gold:\tPhilips\t|aida:\tPhilips\t|spotlight:\tPhilips |\n"
     ]
    }
   ],
   "source": [
    "article_number=1\n",
    "\n",
    "an_article=processed_both[article_number]\n",
    "doc_id=an_article.identifier\n",
    "print(doc_id)\n",
    "for m in an_article.entity_mentions:\n",
    "    print('|mention: %s\\t|gold:\\t%s\\t|aida:\\t%s\\t|spotlight:\\t%s |' % (m.mention, m.gold_link, m.aida_link, m.spotlight_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

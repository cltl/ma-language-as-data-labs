{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5.2 Extracting attribution relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution relations consists of three components:\n",
    "\n",
    "<ol>\n",
    "    <li>source: an explicit source mentioned in the text to which the content is attributed\n",
    "    <li>cue: a phrase that indicates an attribution relation between a source and some content\n",
    "    <li>content: a phrase or phrases that represent the content attributed to the source\n",
    "</ol>\n",
    "\n",
    "The source has to be a person or organisation, whereas the cue is often a so-called speech act or cognitive verb, e.g. ''say'', ''claim'', 'state'', ''announce'', ''think'', ''believe''. A good strategy to detect attribution relations in sentences is therefore first finding the such predicates and next to get the subjects as human sources: people or organisations.\n",
    "\n",
    "In this notebook, we are going to demonstrate a simple approach to extract attribution relations from texts in three steps:\n",
    "\n",
    "<ol>\n",
    "    <li>Get the predicate, subject and complement from a sentence\n",
    "    <li>Select source introducing predicates only\n",
    "    <li>Select sources that refer to humans aor organizations\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get events, their subjects and complements from spaCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to design a strategy to get events, subjects and complements from syntax, we are first going to analyse two sentences that reflect attribution relation and which are processed by spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# depending on how you installed spaCy, the name of the model might be different\n",
    "nlp = spacy.load(name='en_core_web_sm') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at two example sentences that contain an attribution relation to see what dependency structure spaCy yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0cc4bc9f1b1e4f4aaa60ba31cc066b45-0\" class=\"displacy\" width=\"3550\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Google</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">self-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">driving</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">car</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">division</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">announces</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">they</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">meet</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">their</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">target.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">Investors</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">did</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">not</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">believe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">true.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,2.0 925.0,2.0 925.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-1\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M205.0,441.5 L213.0,429.5 197.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,264.5 910.0,264.5 910.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-5\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,177.0 1790.0,177.0 1790.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-7\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,264.5 1785.0,264.5 1785.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-8\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,441.5 L1637,429.5 1653,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-9\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,89.5 1795.0,89.5 1795.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,441.5 L1803.0,429.5 1787.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-10\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,441.5 L1987,429.5 2003,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-11\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,264.5 2135.0,264.5 2135.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2135.0,441.5 L2143.0,429.5 2127.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-12\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,177.0 2840.0,177.0 2840.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,441.5 L2337,429.5 2353,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-13\" stroke-width=\"2px\" d=\"M2520,439.5 C2520,264.5 2835.0,264.5 2835.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,441.5 L2512,429.5 2528,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-14\" stroke-width=\"2px\" d=\"M2695,439.5 C2695,352.0 2830.0,352.0 2830.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,441.5 L2687,429.5 2703,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-15\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,352.0 3180.0,352.0 3180.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,441.5 L3037,429.5 3053,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-16\" stroke-width=\"2px\" d=\"M2870,439.5 C2870,264.5 3185.0,264.5 3185.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3185.0,441.5 L3193.0,429.5 3177.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-17\" stroke-width=\"2px\" d=\"M3220,439.5 C3220,352.0 3355.0,352.0 3355.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0cc4bc9f1b1e4f4aaa60ba31cc066b45-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3355.0,441.5 L3363.0,429.5 3347.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example=\"Google's self-driving car division announces that they will meet their target. Investors did not believe it is true.\"\n",
    "doc = nlp(example)\n",
    "displacy.render(doc, jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the verbs *announe* and *believe* represent the cues for an attribution relation. The subjects of these verbs are the sources and their complements are the content that is attributed to the source. If we want to obtain these three elements, we need to define the steps to extract these:\n",
    "\n",
    "<ol>\n",
    "    <li>Obtain the main predicates from a sentence\n",
    "    <li>For each predicate get the subject as the source and get the remainder of the sentence that has a dependency to the predicate (the complement) as the content\n",
    "</ol>\n",
    "\n",
    "If you design a plan like this, think about the factors that may complicate this, e.g. more than one clause and predicate in the same sentence, ellipsis, conjunctions, complex subject phrases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a similar function as we used before to get tuples with the predicate, the subject and now also the full complement of the predicate. Another difference with the function that we used for extracting events, is that we now want to operate this function on each sentence and not all tokens in a document.\n",
    "\n",
    "A complicated factor is that we do not just want the dependency relations between the heads of the constituents but that we also want to have all the other tokens of the constituents that depend on the verb. For example for the above sentence, the source is \"Google's self-driving car division\", while the dependency is between the verb *announce* and the noun *division*. Similarly, we want to have the full that-clause as a the complement of *announce* and not just the relation between *announce* and *meet*.\n",
    "\n",
    "What we want is the following tuple:\n",
    "\n",
    "```\n",
    "[announce, Google's self-driving car division, that they will meet their target]\n",
    "```\n",
    "\n",
    "We therefore need a function, which we call *get_dependent_tokens*, that collects all the tokens governed by a head. So for the main verb we want to be able to collect all the tokens with a dependency but also the tokens that depend on this dependency. We thus need a recursive function that collects all tokens dominated by the main verb at any depth. Below, we give such a recursive function that given a sentence and the token id of a head, 1) collects all tokens with a dependency relation but also calls the same function again to proceed to the tokens that depend on these dependent tokens. The function continues recursively to collect tokens with further dependencies until it does not find any tokens any more.\n",
    "\n",
    "There is one special for getting the attribution. We are extracting the subject constituent separately from the complement tokens of the main predicate. We therefore add an extra parameter to provide a dependent token that should be excluded. Below you find our function that takes the identifier of the head (head_id), the excluded identifier (for the subject in our case) and the sentence object from spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function that takes the token id of the head to get all tokens that directly or indirectly depend on it given a spaCy sentence object\n",
    "# There is an additional parameter exclude_id to indicate which dependent constituent should be excluded.\n",
    "# The result is a list of tokens\n",
    "\n",
    "def get_dependent_tokens(head_id, exclude_id, sent):\n",
    "    tokens=[]\n",
    "    for token in sent:\n",
    "        ### check if this token is not the same as the head token itself nor the excluded token\n",
    "        if token.i!=head_id and token.i!=exclude_id:\n",
    "            head = token.head\n",
    "            ### check of this token is indeed dependent on the head token\n",
    "            if (head_id==head.i):\n",
    "                ### we want this token and put it in the result list\n",
    "                tokens.append(token.i)\n",
    "                ### we recursively call the function again with our new token to see if there are other tokens that depend on the new token\n",
    "                nested_tokens=get_dependent_tokens(token.i, exclude_id, sent)\n",
    "                ### if we have a result, we extend the result list with the deeper tokens\n",
    "                if nested_tokens:\n",
    "                    tokens.extend(nested_tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*WARNING!!!*\n",
    "Recursive functions are very elegant and powerful, but they are also very dangerous. They could go on for ever for certain input. For example, if the condition create a circular relation in which e.g. condition *a* yields condition *b* and *b* yields *a*. So be careful creating and applying resursive functions. If you end up in an infinite-loop, either kill the function when running, or wait until you get an out-of-memory-error. Don't worry, nothing breaks but you may want to relaunch the notebook or the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the above recursive function in a function *get_predicate_subject_complement_phrases* to get the predicates, the subject phrase and the complement phrase from a spaCy sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prarameter is a spaCy sentence\n",
    "def get_predicate_subject_complement_phrases(doc, sent):\n",
    "    \"\"\"\n",
    "    extract predicates with:\n",
    "    -subject phrase\n",
    "    -complement phrase\n",
    "    \n",
    "    :param spacy.tokens.Sent sent: spaCy object after processing text\n",
    "    \n",
    "    :rtype: list \n",
    "    :return: list of tuples (predicate, subject, complement)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### result list that is returned\n",
    "    output = []\n",
    "    \n",
    "    ### we use a dictionary to collect all predicates and their corresponding subjects if any\n",
    "    predicates = {}\n",
    "    \n",
    "    ### We first get the token that has a nsubj dependency with the main verb\n",
    "    for token in sent:\n",
    "        if (token.dep_=='nsubj'):\n",
    "            predicates[token.head.i] = token.i\n",
    "    \n",
    "    ### Note that the next loop is not executed if there are no predicates with such a subject.\n",
    "    for pred_token, pred_info in predicates.items():\n",
    "        ## We get the subject identifier for this predicate\n",
    "        subject_id = pred_info\n",
    "        ### We get all the tokens that make up the subject phrase\n",
    "        subject_tokens = get_dependent_tokens(subject_id, pred_token, sent)\n",
    "        subject_tokens.extend([subject_id])\n",
    "        ### We sort the tokens to get them in the right order\n",
    "        subject_tokens.sort()\n",
    "        ### We get the full phrase from the subject tokens\n",
    "        subject_phrase = \"\"\n",
    "        for token in subject_tokens:\n",
    "            subject_phrase+=\" \"+doc[token].text\n",
    "        \n",
    "        ### We get all the tokens that make up the complement phrase, we exclude the subject\n",
    "        complement_tokens=get_dependent_tokens(pred_token, subject_id, sent)\n",
    "        ### We sort the phrase to get the tokens in the right order\n",
    "        complement_tokens.sort()\n",
    "        \n",
    "        if complement_tokens:\n",
    "            complement_phrase = \"\"\n",
    "            for token in complement_tokens:\n",
    "                complement_phrase+=\" \"+doc[token].text\n",
    "            one_row = (doc[pred_token].lemma_,\n",
    "                       subject_phrase,\n",
    "                       complement_phrase\n",
    "                      )\n",
    "            output.append(one_row)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our function on the example sentences to see if we get the right tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google's self-driving car division announces that they will meet their target.\n",
      "('announce', \" Google 's self - driving car division\", ' that they will meet their target .')\n",
      "('meet', ' they', ' that will their target')\n",
      "Investors did not believe it is true.\n",
      "('believe', ' Investors', ' did not it is true .')\n",
      "('be', ' it', ' true')\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    events = get_predicate_subject_complement_phrases(doc, sent)\n",
    "    if events:\n",
    "        for event in events:\n",
    "            print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply it to a complete document and aggregate the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Change the path to your own text file\n",
    "path_to_file='../lab1-getting-text/techcrunch_search_results/apple%20os%20x17.txt'\n",
    "events=[]\n",
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        events.extend(get_predicate_subject_object(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the result, we can see that we overgenerate tuples for attribution relations. In the next section, we will try to filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('work', 'Sarah', ' currently as a writer for TechCrunch , after having previously spent over three years at ReadWriteWeb .'), ('work', 'Sarah', ' Prior to her work as a reporter , in I.T. across a number of industries , including banking , retail and software .'), ('offer', 'Latest', ' now curbside alcohol pickup at 2,000 US stores Oct 30 , 2019'), ('continue', 'wars', ' .'), ('make', 'Amazon', ' this week just grocery delivery free , so Walmart is now touting how its grocery service offers the booze .'), ('tout', 'Walmart', ' is now how its grocery service offers the booze .'), ('offer', 'service', ' how the booze'), ('announce', 'retailer', ' today a new milestone in  '), ('wipe', 'bug', ' out over 20 M ratings Oct 30 , 2019'), ('remove', 'sweep', ' more than 20 million ratings from the most popular apps — including from well - known brands like Google , Microsoft , Starbucks , Hulu , Nike and others & Apple TV+ will be free with an Apple Music student subscription Oct 30 , 2019'), ('announce', 'company', ' Sarah Perez Ahead of Friday ’s launch of Apple ’s new streaming service , Apple TV+ , an Apple Music / Apple TV+ bundle deal specifically aimed at making the service more affordable fo Spotify launches a dedicated Kids app for Premium Family'), ('affordable', 'service', ' more'), ('launch', 'Spotify', ' fo a dedicated Kids app for Premium Family'), ('announce', 'Spotify', ' In a move to boost family subscriptions to its app , this morning the launch of a dedicated Kids application which allows children three and up to listen to their own music , both onli HBO Max will cost $ 14.99 per month and launch in May 2020'), ('allow', 'which', ' children three and up to listen to their own music'), ('three', 'children', ' and up'), ('onli', 'both', ' HBO Max will cost $ 14.99 per month and launch in May 2020'), ('cost', 'Max', ' will $ 14.99 per month and launch in May 2020'), ('cost', 'service', ' will $ 14.99 per month —  '), ('establish', 'Walmart', ' to jointly a new fintech accelerator , Tailfin Labs Oct 29 , 2019'), ('announce', 'Walmart', ' today an expansion of its existing relationship with financial services provider Green Dot , which will continue to serve as the issuing bank and program manager for the Walmart Money Sony to shut down PlayStation Vue on January 30 , 2020 Oct 29 , 2019 Sarah Perez Sony ’s live TV streaming service , PlayStation Vue , is shutting down .'), ('continue', 'which', ' will to serve as the issuing bank and program manager for the Walmart Money Sony to shut down PlayStation Vue on January 30 , 2020 Oct 29 , 2019 Sarah Perez Sony ’s live TV streaming service , PlayStation Vue , is shutting down'), ('shut', 'service', ' is down'), ('be', 'service', ' will no longer available as of January 30 , 2020'), ('announce', 'company', ' The service will no longer be available as of January 30 , 2020 , today .'), ('come', 'news', ' only days after r'), ('bring', 'Google', ' its ‘ .new ’ domains to the rest of the web , including to Spotify , Microsoft & others Oct 29 , 2019'), ('roll', 'Google', ' Sarah Perez A year ago , out “ .new'), ('work', 'that', ' like shortcuts to instantly create new Google documents'), ('type', 'you', ' For example , could “ doc.new ” ( without the quotes ) to cr Venmo'), ('announce', 'Venmo', ' , today its first - ever rewards program , Venmo Rewards , which will allow users to earn automatic cash back on purchases when they'), ('allow', 'which', ' will users to earn automatic cash back on purchases when they'), ('earn', 'users', ' to automatic cash back on purchases when they'), ('take', 'GameClub', ' , on Apple Arcade Oct 26 , 2019'), ('recap', 'that', ' the latest OS news'), ('flow', 'that', ' through it all'), ('see', 'industry', ' \\xa0'), ('bil', '194', ' GameClub offers mobile gaming ’s greatest hits for $ 5 per month Oct 24 , 2019'), ('offer', 'GameClub', ' mobile gaming ’s greatest hits for $ 5 per month'), ('introduce', 'Arcade', ' the idea of all - you - can - eat subscription - based mobile gaming to the mainstream .'), ('follow', 'Pass', ' soon as a way to subscribe to a sizable collection of both apps and ga Spotify now lets artists buy a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019'), ('let', 'ga', ' now artists buy a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019'), ('buy', 'artists', ' a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019'), ('add', 'Spotify', ' recently a feature that will occasionally pop up a full - screen recommendation of a new album the service thinks you ’ll like , based on a combination of your listening taste and huma Apple TV app comes to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019'), ('pop', 'that', ' will occasionally up a full - screen recommendation of a new album'), ('think', 'service', ' you ’ll like , based on a combination of your listening taste and huma Apple TV app comes to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019'), ('like', 'you', ' ’ll , based on a combination of your listening taste'), ('come', 'app', ' to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019'), ('begin', 'app', ' Sarah Perez Ahead of the launch of Apple TV+ on November 1 , has to roll out to other platforms beyond Apple ’s own streaming media player , Mac computers and iOS devices .'), ('raise', 'Current', ' $ 20 M Series B , tops half a million users'), ('expand', 'Current', ' to offer personal checking accounts earlier this year .'), ('begin', 'which', ' as a teen debit card controlled by parents'), ('say', 'company', ' Now it has grown to host more than  '), ('grow', 'it', ' has to host more than  '), ('debut', 'Google', ' new digital well - being'), ('expand', 'company', ' , has since the feature set to include new options , like Just 6 % of US adults on Twitter account for 73 % of political tweets … and they disapprove of Trump Oct 23 , 2019'), ('disapprove', 'they', ' of Trump Oct 23 , 2019'), ('create', 'number', ' the majority of tweets , and that extends to Twitter discussions around politics , according to a new report from the Pew Research Center out today .  '), ('extend', 'that', ' to Twitter discussions around politics , according to a new report from the Pew Research Center out today .  '), ('sell', 'Quibi', ' out of its $ 150 M in first - year ad inventory Oct 23 , 2019'), ('launch', 'Quibi', ' ’s has n’t even , but it ’s already sold out of its $ 150 million first - year advertising inventory'), ('sell', '’s', ' it already out of its $ 150 million first - year advertising inventory'), ('announce', 'company', ' ’s mobile - only streaming service Quibi has n’t even launched , but it ’s already sold out of its $ 150 million first - year advertising inventory , t'), ('teach', 'set', ' users about features'), ('focus', '’s', ' TikTok ’s new set of safety videos teach users about features , on ‘'), ('release', 'TikTok', ' TikTok ’s new set of safety videos teach users about features , the app ’s focus on ‘ positivity ’ Oct 23 , 2019 today a new set of safety videos designed to playfully inform users about the app ’s privacy controls and other features'), ('expand', 'Amazon', ' its in - store pickup service , Counter , to thousands more stores'), ('launch', 'that', ' this summer at 100'), ('expand', 'locations', ' , is now .'), ('announce', 'company', ' today the service will reac Salesforce Ventures ’s John Somorjai warns N.C. ’s politics could dampen its tech hub potential Oct 22 , 2019'), ('reac', 'service', ' will Salesforce Ventures ’s John Somorjai warns N.C. ’s politics could dampen its tech hub potential Oct 22 , 2019'), ('warn', 'Somorjai', ' N.C. ’s politics could dampen its tech hub potential Oct 22 , 2019'), ('dampen', 'politics', ' could its tech hub potential Oct 22 , 2019'), ('rise', 'Carolina', ' has been as an entrepreneurial hub .'), ('’s', 'It', ' now home to massive deals , like IBM buying Red Hat for $ 34 billion and Fortnite maker Epic Games raising a landmark $ 1.25 billion ,   Load More'), ('buy', 'IBM', ' Red Hat for $ 34 billion and Fortnite maker Epic Games')]\n"
     ]
    }
   ],
   "source": [
    "print(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering tuples for cue predictates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get these predicates, we are going to use a list of FrameNet frames that have been hand picked as so-called Source-Introducing-Frames. We can use the same function as before for detecting events to first get the predciates with frames and filter the onces that introduce a source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading FrameNet in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## probably already done\n",
    "#nltk.download('framenet_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import framenet as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Loading source introducing frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the list of FrameNet frames in the file sip-frames.txt to filter events in texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sip_file='sip-frames.txt'\n",
    "sip_frames=[]\n",
    "with open(path_to_sip_file) as fp: sip_frames = fp.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We check the number of frames\n",
    "len(sip_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Achieving_first', 'Adding_up', 'Adducing', 'Agree_or_refuse_to_act', 'Appointing', 'Attempt_suasion', 'Bail_decision', 'Be_in_agreement_on_assessment', 'Be_translation_equivalent', 'Become_silent', 'Behind_the_scenes', 'Being_named', 'Body_movement', 'Bragging', 'Categorization', 'Chatting', 'Choosing', 'Claim_ownership', 'Coming_up_with', 'Commitment', 'Communicate_categorization', 'Communication', 'Communication_manner', 'Communication_means', 'Communication_noise', 'Communication_response', 'Compatibility', 'Complaining', 'Compliance', 'Confronting_problem', 'Contacting', 'Criminal_investigation', 'Deny_permission', 'Deserving', 'Discussion', 'Distinctiveness', 'Encoding', 'Eventive_cognizer_affecting', 'Evidence', 'Experiencer_obj', 'Expressing_publicly', 'Forgiveness', 'Gesture', 'Grant_permission', 'Have_as_translation_equivalent', 'Heralding', 'Imposing_obligation', 'Judgment', 'Judgment_communication', 'Judgment_direct_address', 'Justifying', 'Labeling', 'Linguistic_meaning', 'Make_agreement_on_action', 'Make_noise', 'Making_faces', 'Manipulate_into_doing', 'Motion_noise', 'Name_conferral', 'Notification_of_charges', 'Omen', 'Pardon', 'Predicting', 'Prevarication', 'Prohibiting', 'Questioning', 'Referring_by_name', 'Regard', 'Reporting', 'Request', 'Respond_to_proposal', 'Reveal_secret', 'Rite', 'Seeking', 'Sign', 'Silencing', 'Simple_naming', 'Speak_on_topic', 'Spelling_and_pronouncing', 'Statement', 'Suasion', 'Subjective_influence', 'Successfully_communicate_message', 'Talking_into', 'Telling', 'Text_creation', 'Verdict', 'Appearance', 'Categorization', 'Chemical-sense_description', 'Locating', 'Perception_active', 'Perception_body', 'Perception_experience', 'Seeking', 'Trust', 'Adopt_selection', 'Assessing', 'Awareness', 'Becoming_aware', 'Categorization', 'Cause_emotion', 'Certainty', 'Choosing', 'Cogitation', 'Coming_to_believe', 'Daring', 'Desiring', 'Differentiation', 'Emotion_active', 'Estimating', 'Expectation', 'Experiencer_focus', 'Experiencer_obj', 'Familiarity', 'Feeling', 'Feigning', 'Grasp', 'Importance', 'Judgment', 'Occupy_rank', 'Opinion', 'Partiality', 'Place_weight_on', 'Preference', 'Purpose', 'Reliance', 'Scrutiny', 'Seeking', 'Taking_sides', 'Topic']\n"
     ]
    }
   ],
   "source": [
    "print(sip_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply iterate over the tuples and check if the associated frames match any of the SiFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_event=[]\n",
    "for event_tuple in events:\n",
    "    frames =  fn.frames_by_lemma(event_tuple[0])\n",
    "    sip_frame=\"\"\n",
    "    if frames:\n",
    "        for frame in frames:\n",
    "            if frame.name in sip_frames:\n",
    "                sip_frame = frame.name\n",
    "                break\n",
    "    if sip_frame:\n",
    "        filtered_event.append(event_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('work', 'Sarah', ' currently as a writer for TechCrunch , after having previously spent over three years at ReadWriteWeb .')\n",
      "('work', 'Sarah', ' Prior to her work as a reporter , in I.T. across a number of industries , including banking , retail and software .')\n",
      "('make', 'Amazon', ' this week just grocery delivery free , so Walmart is now touting how its grocery service offers the booze .')\n",
      "('tout', 'Walmart', ' is now how its grocery service offers the booze .')\n",
      "('announce', 'retailer', ' today a new milestone in  ')\n",
      "('announce', 'company', ' Sarah Perez Ahead of Friday ’s launch of Apple ’s new streaming service , Apple TV+ , an Apple Music / Apple TV+ bundle deal specifically aimed at making the service more affordable fo Spotify launches a dedicated Kids app for Premium Family')\n",
      "('announce', 'Spotify', ' In a move to boost family subscriptions to its app , this morning the launch of a dedicated Kids application which allows children three and up to listen to their own music , both onli HBO Max will cost $ 14.99 per month and launch in May 2020')\n",
      "('allow', 'which', ' children three and up to listen to their own music')\n",
      "('announce', 'Walmart', ' today an expansion of its existing relationship with financial services provider Green Dot , which will continue to serve as the issuing bank and program manager for the Walmart Money Sony to shut down PlayStation Vue on January 30 , 2020 Oct 29 , 2019 Sarah Perez Sony ’s live TV streaming service , PlayStation Vue , is shutting down .')\n",
      "('shut', 'service', ' is down')\n",
      "('be', 'service', ' will no longer available as of January 30 , 2020')\n",
      "('announce', 'company', ' The service will no longer be available as of January 30 , 2020 , today .')\n",
      "('come', 'news', ' only days after r')\n",
      "('roll', 'Google', ' Sarah Perez A year ago , out “ .new')\n",
      "('work', 'that', ' like shortcuts to instantly create new Google documents')\n",
      "('type', 'you', ' For example , could “ doc.new ” ( without the quotes ) to cr Venmo')\n",
      "('announce', 'Venmo', ' , today its first - ever rewards program , Venmo Rewards , which will allow users to earn automatic cash back on purchases when they')\n",
      "('allow', 'which', ' will users to earn automatic cash back on purchases when they')\n",
      "('earn', 'users', ' to automatic cash back on purchases when they')\n",
      "('take', 'GameClub', ' , on Apple Arcade Oct 26 , 2019')\n",
      "('see', 'industry', ' \\xa0')\n",
      "('bil', '194', ' GameClub offers mobile gaming ’s greatest hits for $ 5 per month Oct 24 , 2019')\n",
      "('follow', 'Pass', ' soon as a way to subscribe to a sizable collection of both apps and ga Spotify now lets artists buy a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019')\n",
      "('let', 'ga', ' now artists buy a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019')\n",
      "('add', 'Spotify', ' recently a feature that will occasionally pop up a full - screen recommendation of a new album the service thinks you ’ll like , based on a combination of your listening taste and huma Apple TV app comes to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n",
      "('think', 'service', ' you ’ll like , based on a combination of your listening taste and huma Apple TV app comes to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n",
      "('like', 'you', ' ’ll , based on a combination of your listening taste')\n",
      "('come', 'app', ' to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n",
      "('raise', 'Current', ' $ 20 M Series B , tops half a million users')\n",
      "('say', 'company', ' Now it has grown to host more than  ')\n",
      "('grow', 'it', ' has to host more than  ')\n",
      "('disapprove', 'they', ' of Trump Oct 23 , 2019')\n",
      "('announce', 'company', ' ’s mobile - only streaming service Quibi has n’t even launched , but it ’s already sold out of its $ 150 million first - year advertising inventory , t')\n",
      "('announce', 'company', ' today the service will reac Salesforce Ventures ’s John Somorjai warns N.C. ’s politics could dampen its tech hub potential Oct 22 , 2019')\n",
      "('reac', 'service', ' will Salesforce Ventures ’s John Somorjai warns N.C. ’s politics could dampen its tech hub potential Oct 22 , 2019')\n",
      "('rise', 'Carolina', ' has been as an entrepreneurial hub .')\n"
     ]
    }
   ],
   "source": [
    "for event in filtered_event:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not happy with the result, you can adapt the file *sip-frames.txt* to make it more restrictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtering the sources as people and organisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the above list and check whether the subjects of these predicates are indeed people or organizations. In so far they are not, we can build in more filters on the subject. We can also sort the results per subject.\n",
    "\n",
    "Let us first sort the tuple by the presumed source to get a better idea about the source candidates. We define a little function *getKey* that selects the second element from the tuple.\n",
    "We use the *sorted* function to sort the tuple by that second element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "sorted_by_source=sorted(filtered_event,key=getKey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the sorted tuples in the order of the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('194', 'bil', ' GameClub offers mobile gaming ’s greatest hits for $ 5 per month Oct 24 , 2019')\n",
      "('Amazon', 'make', ' this week just grocery delivery free , so Walmart is now touting how its grocery service offers the booze .')\n",
      "('Carolina', 'rise', ' has been as an entrepreneurial hub .')\n",
      "('Current', 'raise', ' $ 20 M Series B , tops half a million users')\n",
      "('GameClub', 'take', ' , on Apple Arcade Oct 26 , 2019')\n",
      "('Google', 'roll', ' Sarah Perez A year ago , out “ .new')\n",
      "('Pass', 'follow', ' soon as a way to subscribe to a sizable collection of both apps and ga Spotify now lets artists buy a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019')\n",
      "('Sarah', 'work', ' currently as a writer for TechCrunch , after having previously spent over three years at ReadWriteWeb .')\n",
      "('Sarah', 'work', ' Prior to her work as a reporter , in I.T. across a number of industries , including banking , retail and software .')\n",
      "('Spotify', 'announce', ' In a move to boost family subscriptions to its app , this morning the launch of a dedicated Kids application which allows children three and up to listen to their own music , both onli HBO Max will cost $ 14.99 per month and launch in May 2020')\n",
      "('Spotify', 'add', ' recently a feature that will occasionally pop up a full - screen recommendation of a new album the service thinks you ’ll like , based on a combination of your listening taste and huma Apple TV app comes to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n",
      "('Venmo', 'announce', ' , today its first - ever rewards program , Venmo Rewards , which will allow users to earn automatic cash back on purchases when they')\n",
      "('Walmart', 'tout', ' is now how its grocery service offers the booze .')\n",
      "('Walmart', 'announce', ' today an expansion of its existing relationship with financial services provider Green Dot , which will continue to serve as the issuing bank and program manager for the Walmart Money Sony to shut down PlayStation Vue on January 30 , 2020 Oct 29 , 2019 Sarah Perez Sony ’s live TV streaming service , PlayStation Vue , is shutting down .')\n",
      "('app', 'come', ' to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n",
      "('company', 'announce', ' Sarah Perez Ahead of Friday ’s launch of Apple ’s new streaming service , Apple TV+ , an Apple Music / Apple TV+ bundle deal specifically aimed at making the service more affordable fo Spotify launches a dedicated Kids app for Premium Family')\n",
      "('company', 'announce', ' The service will no longer be available as of January 30 , 2020 , today .')\n",
      "('company', 'say', ' Now it has grown to host more than  ')\n",
      "('company', 'announce', ' ’s mobile - only streaming service Quibi has n’t even launched , but it ’s already sold out of its $ 150 million first - year advertising inventory , t')\n",
      "('company', 'announce', ' today the service will reac Salesforce Ventures ’s John Somorjai warns N.C. ’s politics could dampen its tech hub potential Oct 22 , 2019')\n",
      "('ga', 'let', ' now artists buy a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019')\n",
      "('industry', 'see', ' \\xa0')\n",
      "('it', 'grow', ' has to host more than  ')\n",
      "('news', 'come', ' only days after r')\n",
      "('retailer', 'announce', ' today a new milestone in  ')\n",
      "('service', 'shut', ' is down')\n",
      "('service', 'be', ' will no longer available as of January 30 , 2020')\n",
      "('service', 'think', ' you ’ll like , based on a combination of your listening taste and huma Apple TV app comes to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n",
      "('service', 'reac', ' will Salesforce Ventures ’s John Somorjai warns N.C. ’s politics could dampen its tech hub potential Oct 22 , 2019')\n",
      "('that', 'work', ' like shortcuts to instantly create new Google documents')\n",
      "('they', 'disapprove', ' of Trump Oct 23 , 2019')\n",
      "('users', 'earn', ' to automatic cash back on purchases when they')\n",
      "('which', 'allow', ' children three and up to listen to their own music')\n",
      "('which', 'allow', ' will users to earn automatic cash back on purchases when they')\n",
      "('you', 'type', ' For example , could “ doc.new ” ( without the quotes ) to cr Venmo')\n",
      "('you', 'like', ' ’ll , based on a combination of your listening taste')\n"
     ]
    }
   ],
   "source": [
    "for tuple in sorted_by_source:\n",
    "    source_tuple=(tuple[1], tuple[0], tuple[2])\n",
    "    print(source_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the sources? How do you think you can filter tuples based on the source being human?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of solving this is by checking if the subject is a named entity and spaCy assigned the type PERSON or ORG. For this, we extend the tuple with an entity type if there is any. Luckily, spaCy allows us to iterate over the subject tokens and check if an entity label is assigned. Below is the adapted function with the extended tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prarameter is a spaCy sentence\n",
    "def get_predicate_subject_type_complement_phrases(doc, sent):\n",
    "    \"\"\"\n",
    "    extract predicates with:\n",
    "    -subject phrase\n",
    "    -complement phrase\n",
    "    \n",
    "    :param spacy.tokens.Sent sent: spaCy object after processing text\n",
    "    \n",
    "    :rtype: list \n",
    "    :return: list of tuples (predicate, subject, complement)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### result list that is returned\n",
    "    output = []\n",
    "    \n",
    "    ### we use a dictionary to collect all predicates and their corresponding subjects if any\n",
    "    predicates = {}\n",
    "    \n",
    "    ### We first get the token that has a nsubj dependency with the main verb\n",
    "    for token in sent:\n",
    "        if (token.dep_=='nsubj'):\n",
    "            predicates[token.head.i] = token.i\n",
    "    \n",
    "    ### Note that the next loop is not executed if there are no predicates with such a subject.\n",
    "    for pred_token, pred_info in predicates.items():\n",
    "        ## We get the subject identifier for this predicate\n",
    "        subject_id = pred_info\n",
    "        ### We get all the tokens that make up the subject phrase\n",
    "        subject_tokens = get_dependent_tokens(subject_id, pred_token, sent)\n",
    "        subject_tokens.extend([subject_id])\n",
    "        ### We sort the tokens to get them in the right order\n",
    "        subject_tokens.sort()\n",
    "        ### We get the full phrase from the subject tokens\n",
    "        subject_phrase = \"\"\n",
    "        for token in subject_tokens:\n",
    "            subject_phrase+=\" \"+doc[token].text\n",
    "        \n",
    "        ### We define a variable to store the entity label for the subject tokens if any\n",
    "        ent_label = \"\"\n",
    "            \n",
    "        for token in subject_tokens:\n",
    "            ent_label =doc[token].ent_type_\n",
    "            ### if we have a label, we can break\n",
    "            if ent_label:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        ### We get all the tokens that make up the complement phrase, we exclude the subject\n",
    "        complement_tokens=get_dependent_tokens(pred_token, subject_id, sent)\n",
    "        ### We sort the phrase to get the tokens in the right order\n",
    "        complement_tokens.sort()\n",
    "        \n",
    "        if complement_tokens:\n",
    "            complement_phrase = \"\"\n",
    "            for token in complement_tokens:\n",
    "                complement_phrase+=\" \"+doc[token].text\n",
    "            one_row = (doc[pred_token].lemma_,\n",
    "                       subject_phrase,\n",
    "                       ent_label,\n",
    "                       complement_phrase\n",
    "                      )\n",
    "            output.append(one_row)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply our new function to get the quadruples from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Change the path to your own text file\n",
    "path_to_file='../lab1-getting-text/techcrunch_search_results/apple%20os%20x17.txt'\n",
    "events=[]\n",
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        events.extend(get_predicate_subject_type_complement_phrases(doc, sent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the frame filter with another filter on the entity label. We also print the subject phrase in case there is no matching type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The retailer\n",
      " the company\n",
      " which\n",
      " Oct 29 , 2019 Sarah Perez Sony ’s live TV streaming service , PlayStation Vue ,\n",
      " The service\n",
      " the company\n",
      " The news\n",
      " that\n",
      " you\n",
      " Venmo\n",
      " which\n",
      " users\n",
      " GameClub\n",
      " The app industry in 2018\n",
      " 194\n",
      " ga Spotify\n",
      " the service\n",
      " you\n",
      " the company\n",
      " it\n",
      " they\n",
      " the company\n",
      " The company\n",
      " the service\n"
     ]
    }
   ],
   "source": [
    "filtered_event=[]\n",
    "for event_tuple in events:\n",
    "    frames =  fn.frames_by_lemma(event_tuple[0])\n",
    "    sip_frame=\"\"\n",
    "    if frames:\n",
    "        for frame in frames:\n",
    "            if frame.name in sip_frames:\n",
    "                sip_frame = frame.name\n",
    "                break\n",
    "    if sip_frame:\n",
    "        if event_tuple[2]=='PERSON' or event_tuple[2]=='ORG':\n",
    "            filtered_event.append(event_tuple)\n",
    "        else:\n",
    "            print(event_tuple[1]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the missed subjects contain subjects we do not want but also good subjects such as *the company* and pronouns *you* and *they*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider the selected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_source=sorted(filtered_event,key=getKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Amazon', 'make', 'ORG', ' this week just grocery delivery free , so Walmart is now touting how its grocery service offers the booze .')\n",
      "(' Earl Mobile banking app Current', 'raise', 'PERSON', ' $ 20 M Series B , tops half a million users')\n",
      "(' Google', 'roll', 'ORG', ' Sarah Perez A year ago , out “ .new')\n",
      "(' Google Play Pass', 'follow', 'ORG', ' soon as a way to subscribe to a sizable collection of both apps and ga Spotify now lets artists buy a full - screen ‘ recommendation ’ promoting their new album Oct 24 , 2019')\n",
      "(' Sarah', 'work', 'PERSON', ' Prior to her work as a reporter , in I.T. across a number of industries , including banking , retail and software .')\n",
      "(' Sarah Perez North Carolina', 'rise', 'PERSON', ' has been as an entrepreneurial hub .')\n",
      "(' Sarah Perez Spotify', 'add', 'PERSON', ' recently a feature that will occasionally pop up a full - screen recommendation of a new album the service thinks you ’ll like , based on a combination of your listening taste and huma Apple TV app comes to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n",
      "(' Sarah Perez Walmart', 'announce', 'PERSON', ' today an expansion of its existing relationship with financial services provider Green Dot , which will continue to serve as the issuing bank and program manager for the Walmart Money Sony to shut down PlayStation Vue on January 30 , 2020 Oct 29 , 2019 Sarah Perez Sony ’s live TV streaming service , PlayStation Vue , is shutting down .')\n",
      "(' Sarah Perez Writer Sarah', 'work', 'PERSON', ' currently as a writer for TechCrunch , after having previously spent over three years at ReadWriteWeb .')\n",
      "(' Spotify', 'announce', 'PERSON', ' In a move to boost family subscriptions to its app , this morning the launch of a dedicated Kids application which allows children three and up to listen to their own music , both onli HBO Max will cost $ 14.99 per month and launch in May 2020')\n",
      "(' Walmart', 'tout', 'PERSON', ' is now how its grocery service offers the booze .')\n",
      "(' huma Apple TV app', 'come', 'ORG', ' to Amazon ’s Fire TV Stick and other devices Oct 24 , 2019')\n"
     ]
    }
   ],
   "source": [
    "for tuple in sorted_by_source:\n",
    "    source_tuple=(tuple[1], tuple[0], tuple[2], tuple[3])\n",
    "    print(source_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how to further improve the results. For now, we save the current result to use it for the next notebook on sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('attribution-relations.pickle', 'wb') as outputfile:\n",
    "    pickle.dump(sorted_by_source, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
